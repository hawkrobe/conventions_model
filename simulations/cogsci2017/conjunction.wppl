///fold:
var initList = function(n, val) {
  return repeat(n, function() {return val})
}

var uniformPs = function(vs) {
  return initList(vs.length, 1/vs.length)
}
///

// possible states of the world
var states = ['t1', 't2'];
var statePrior =  Categorical({vs: states, ps: [1/2, 1/2]});

// possible base utterances, and possible conjunctions
var unconstrainedUtterances = ['t1_a', 't1_b','t2_a','t2_b'];
var derivedUtterances = ['t1_a t1_b', 't2_a t2_b', 
                         't1_b t2_a', 't1_a t2_a', 't1_a t2_b', 't1_b t2_b'];
var utterances = unconstrainedUtterances.concat(derivedUtterances);
var utterancePrior = Categorical({vs: utterances, ps: uniformPs(utterances)});

// takes a sample from a (biased & discretized) dirichlet distribution for each word,
// representing the extent to which that word describes each object
var lexiconPrior = Infer({method: 'enumerate'}, function(){
  var meanings = map(function(utt) {
    var t1Bias = utt.split('_')[0] === 't1' 
    var t1ps = t1Bias ? [.1,.15,.2,.25,.3] : [.3,.25,.2,.15,.1]
    var t1Prob = categorical({vs: [0.01, 0.25, .5, .75, .99], ps: t1ps})
    return {'t1' : t1Prob, 't2' : 1-t1Prob};
  }, unconstrainedUtterances);
  return _.zipObject(unconstrainedUtterances, meanings);
});

// speaker optimality
var alpha = 15;

// length-based cost 
var uttCost = function(utt) {
  return utt.split(' ').length;
};

// Looks up the meaning of an utterance in a lexicon object  
var uttFitness = cache(function(utt, state, lexicon) {
  return Math.log(reduce(function(subUtt, memo) {
    return lexicon[subUtt][state] * memo;
  }, 1, utt.split(' ')));
});

// literal listener
var L0 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    factor(uttFitness(utt, state, lexicon));
    return state;
  });
});

// pragmatic speaker
var S1 = cache(function(state, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var utt = sample(utterancePrior);
    factor(alpha * (L0(utt, lexicon).score(state))
           - uttCost(utt));
    return utt;
  });
});

// conventional listener
var L1 = cache(function(utt, lexicon) {
  return Infer({method:"enumerate"}, function(){
    var state = sample(statePrior);
    observe(S1(state, lexicon), utt);
    return state;
  });
});

var lexiconPosterior = cache(function(originAgent, data) {
  return Infer({method: 'enumerate'}, function() {
    var lexicon = sample(lexiconPrior);
    mapData({data: data}, function(datum){
      if(originAgent === 'L') {
        observe(S1(datum.obj, lexicon), datum.utt);
      } else if(originAgent === 'S') {
        observe(L1(datum.utt, lexicon), datum.obj);
      }
    });
    return lexicon;
  });
});

// conventional listener (L1, marginalizing over lexicons)
var L = function(utt, data) {
  return Infer({method:"enumerate"}, function(){
    var lexicon = sample(lexiconPosterior('L', data));
    var state = sample(L1(utt, lexicon));
    return state;
  });
};

// conventional speaker (S1, reasoning about expected listener across lexicons)
var S = function(state, data) {
  return Infer({method:"enumerate"}, function(){
    var utt = sample(utterancePrior);
    var listener = Infer({method: 'enumerate'}, function() {
      var lexicon = sample(lexiconPosterior('S', data));
      return sample(L1(utt, lexicon));
    });
    factor(alpha * (listener.score(state))
           - uttCost(utt));
    return utt;
  });
};

console.log("likelihood of t1_a meaning t1: " + 
            expectation(lexiconPosterior('S', []), 
                        function(v) {return v['t1_a']['t1']}))
//viz(S('t1', []))

console.log("likelihood of t1_a meaning t1 after observations: " + 
            expectation(lexiconPosterior('S', [{utt: 't1_a t1_b', obj: 't1'},
                                               {utt: 't1_a', obj: 't1'}]), 
                        function(v) {return v['t1_a']['t1']}))
// viz(S('t1', [{utt: 't1_a t1_b', obj: 't1'},
//              {utt: 't1_a', obj: 't1'}]))
