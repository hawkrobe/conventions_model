// run using, e.g.:
// webppl partnerspecificity.wppl --require ./refModule/ --require webppl-csv

var numUtterances = 4;
var numStates = 2;
var lexDims = [numUtterances, numStates];

var primitiveUtterances = map(function(i) {return 'word' + i;}, _.range(1, numUtterances+1));
var states = map(function(i) {return 'object' + i;}, _.range(1, numStates+1));

// 7 -> 2 works, but has a weird blip at the beginning where it starts low-ish then rises then falls...
var params = {
  alpha : argv.alpha,
  costWeight: argv.costWeight,
  model: argv.model,
  context: ['object1', 'object2'],
  primitiveUtterances: primitiveUtterances,
  states : states,
  utterances : (['word1_word2', 'word3_word4'])
                .concat(primitiveUtterances)
};
console.log(params);

var tensorSoftplus = function(x) {
  return T.log(T.add(T.exp(x), 1));
};

var S = function(state, partnerID) {
  return Infer({method: "enumerate"}, function() {
    var utt = uniformDraw(params.utterances);
    var utility = expectation(Infer({method:"forward", samples: 40000, guide:true}, function(){
      var lexicon = sampleLexicon(partnerID);
      return (params.alpha * refModule.getListenerScore(state, utt, extend(params, {lexicon: lexicon}))
              - params.costWeight * utt.split('_').length);
    }));
    display(utt);
    display(utility);
    factor(utility);
    return utt;
  });
};
var observeRound = function(llex, datum) {
  factor(refModule.getListenerScore(
    datum.clickedName, datum.wordID, extend({}, params, {lexicon: llex})
  ));
};

// for each point in data, we want the model's predictions 
var iterate = function(outputFile, remainingTrials, dataSoFar) {
  console.log('iterating with ' + JSON.stringify(dataSoFar));
  // run VI on current data
  if(!_.isEmpty(dataSoFar)) {
    Optimize({
      steps: 50000, verbose: false, optMethod: {adam: {stepSize: 0.001}},
      model: function() {
	var lexica = {
          1 : sampleLexicon(1),
	  2 : sampleLexicon(2),
	  3 : sampleLexicon(3),
	  4 : sampleLexicon(4)
        };
	mapData({data: dataSoFar}, function(trialDatum) {
	  var i = trialDatum.partnerID;
	  var lexicon = lexica[i];
	  observeRound(lexicon, trialDatum);
	});
      }
    });
  }

  // get marginal prediction of next data point over lexicon posterior
  var currTrial = first(remainingTrials);
  var speakerOutput = S(currTrial.target, currTrial.partnerID);
  console.log(JSON.stringify(speakerOutput));
  var nextUtt = sample(speakerOutput);
  console.log('chosen utt for partner ' + currTrial.partnerID + ' is: ' + nextUtt);
  csv.writeLine([argv.model, argv.chainNum, params.alpha, params.costWeight, currTrial.t, currTrial.partnerID, speakerOutput.score('word1_word2')].join(','), outputFile);
  if(!_.isEmpty(rest(remainingTrials))) {
    iterate(outputFile, rest(remainingTrials), dataSoFar.concat({
      partnerID: currTrial.partnerID,
      target: currTrial.target,
      clickedName: currTrial.target,   // assume for now listener always guesses correct
      t: currTrial.t,
      wordID: nextUtt
    }));
  }
};

var trials = [{t: 1, partnerID: 1, target: 'object1'},
	      {t: 2, partnerID: 1, target: 'object1'},
	      {t: 3, partnerID: 1, target: 'object1'},
	      {t: 4, partnerID: 1, target: 'object1'},
	      {t: 5, partnerID: 2, target: 'object1'},
	      {t: 6, partnerID: 2, target: 'object1'},
	      {t: 7, partnerID: 2, target: 'object1'},
	      {t: 8, partnerID: 2, target: 'object1'},
	      {t: 9, partnerID: 3, target: 'object1'},
	      {t: 10, partnerID: 3, target: 'object1'},
              {t: 11, partnerID: 3, target: 'object1'},
              {t: 12, partnerID: 3, target: 'object1'},
              {t: 14, partnerID: 4, target: 'object1'},                                                        
             ];

var f = csv.open('./output/speakerOutput' + argv.model + argv.chainNum + '.csv');
csv.writeLine('model,chainNum,alpha,costWeight,time,partnerID,prediction',f);
iterate(f, trials, []);
csv.close(f);
