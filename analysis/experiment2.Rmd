---
title: "Basic-level emergence"
output:
  pdf_document: default
  html_notebook: default
  html_document: 
    smart: false
    
---

# Import libraries

```{r results="hide"}
library(tidyverse)
library(ggthemes)
library(lme4)
library(lmerTest)
```

# Import data  

Import, filter out nonConvergers, pull in condition information

```{r results="hide"}
raw_clicks = read_delim('../data/experiment2/allClicks.csv', '\t')
raw_drops = read_delim('../data/experiment2/allDrops.csv', '\t')
incompletes <- (raw_clicks %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  filter(n < 90))$gameid

masterWordIDLookup <- read_delim('../data/experiment2/allWordPostTest.csv', '\t') %>%
  group_by(gameid) %>%
  mutate(wordID = paste0('word', as.numeric(factor(target)))) %>%
  rename(text = target) %>%
  select(gameid, text, wordID) %>%
  distinct()

masterGameIDLookup <- raw_clicks %>%
  mutate(id = paste0('game', as.numeric(factor(gameid)))) %>%
  select(gameid, id, condition) %>%
  distinct()
```

Filter out incompletes & compute cumulative accuracy. We also divide into quarters to compare games that ran different amounts of trials.

```{r}
d <- raw_clicks %>%
  filter(condition != 'mixedLower') %>%
  mutate(acc = ifelse(correct, 1, 0)) %>%
  filter(!(gameid %in% incompletes)) %>%
  group_by(gameid) %>%
  mutate(quarter = floor((trialNum - 1) / (last(trialNum)/4))) %>%
  mutate(cumAcc = cumsum(acc)) %>%
  mutate(overallAcc = last(cumAcc)/last(trialNum)) %>%
  left_join(raw_drops, by = c('gameid', 'trialNum', 'intendedName')) %>%
  select(-ends_with('y'), -ends_with('x'), -correct) %>%
  left_join(masterWordIDLookup) %>%#, by = c('gameid', 'text'))  
  left_join(masterGameIDLookup)

# Exclude people who are below 75% in final quarter
nonConvergers <- (d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  filter(percentCorrect < 0.75))$gameid

cat('excluded', length(nonConvergers), 'games that never converged')
d %>% 
  filter(quarter == 3) %>%
  group_by(gameid, condition) %>%
  summarize(percentCorrect = mean(acc)) %>%
  mutate(toremove = percentCorrect < 0.75) %>%
  group_by(condition, toremove) %>%
  tally()
```

## Number games per condition

```{r}
d %>% 
  group_by(gameid, condition) %>%
  tally() %>%
  group_by(condition) %>%
  summarize(n = length(n))
```

## Write out in nice format for BDA

Want to run these webppl models in parallel, so the input data should be in separate files, easily indexed from the command-line... 

```{r}
gameIDs = d %>% 
  filter(!(gameid %in% nonConvergers)) %>%
  pull(id) %>%
  unique()

for(i in gameIDs) {
  toWrite = d %>% 
    ungroup() %>%
    filter(id == i) %>%
    mutate(speakerID = ifelse(trialNum %% 2 == 0, 1, 2),
           listenerID = ifelse(trialNum %% 2 == 0, 2, 1)) %>%
    select(-gameid, -text, -acc, -quarter, -cumAcc, -overallAcc, -timeFromRoundStart)  
  write_csv(toWrite, path = paste0('../simulations/cogsci2018/input/', i, '.csv'))
}
```

# Behavioral Results 

## Overall accuracy over time

```{r}
d %>% 
  group_by(trialNum) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = trialNum, y = percentCorrect)) +
    geom_point() +
    theme_few() + 
    geom_hline(yintercept = 0.25, linetype = 2) +
    guides(color = FALSE) +
    geom_smooth(method = 'loess') +
    ylab("accuracy") +
    ylim(0,1) 

ggsave('~/Downloads/singleLine.pdf', height = 4, width = 6)
```


## Accuracy by condition

```{r}
d %>%
  mutate(condition = ifelse(condition == 'intermediateOnly', 'coarse', 'fine')) %>%
  group_by(condition, trialNum) %>%
  summarize(trialLevelPctCorrect = mean(acc)) %>%
  ggplot(aes(x = trialNum, y = 100 * trialLevelPctCorrect, color = condition)) +
    geom_point(alpha = 0.2, stroke = 0, size = 2) +
    theme_few() +
    geom_hline(yintercept = 25, lty = 'dashed') +
    geom_smooth(method = 'loess') +
    scale_color_colorblind() +
    theme(aspect.ratio = 1) +
    labs(x = 'trial #', y = '% correct') +
    ylim(0, 100)

ggsave('../writing/journal_manuscript/figures/Exp2_empirical_accuracy.pdf', width = 5, height = 4, 
       useDingbats=FALSE)
```


The overall increase is significant... 

```{r}
trialOnly = glmer(acc ~ trialNum + (1 + trialNum | gameid), family = 'binomial', data = d %>% ungroup())
trialAndCondition = glmer(acc ~ trialNum + condition + (1 + trialNum | gameid), family = 'binomial', data = d %>% ungroup())
anova(trialOnly, trialAndCondition)
```

What is the intercept?

```{r}
t.test((d %>% ungroup() %>% filter(trialNum == 1))$acc, mu = 0.25)
```

## Unique words

```{r}
d %>%
  #filter(!(gameid %in% nonConvergers)) %>%
  mutate(repNum = floor((trialNum - 1) / 8)) %>%
  group_by(gameid, condition, repNum) %>%
  mutate(numUniqueWordsUsed = length(unique(wordID))) %>%
  group_by(condition, repNum) %>%
  tidyboot::tidyboot_mean(numUniqueWordsUsed, nboot= 100) %>%
  ggplot(aes(x = repNum, y = empirical_stat, color = condition)) +
    geom_line() +
    ylim(3.5, 8.5) +
    geom_hline(yintercept = c(4, 8)) +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width=0) +
    theme_few() +
    scale_color_colorblind() +
    theme(aspect.ratio = 1) +
    labs(y = "number unique words used")

ggsave('../writing/journal_manuscript/figures/Exp2_empirical_numwords.pdf', width = 5, height = 4, useDingbats = F)
```

```{r}
d %>%
  mutate(repNum = floor((trialNum - 1) / 8)) %>%
  group_by(gameid, condition, repNum) %>%
  mutate(numUniqueWordsUsed = length(unique(wordID))) %>%
  group_by(condition) %>%
  tidyboot::tidyboot_mean(numUniqueWordsUsed, nboot= 100)

d %>%
  mutate(repNum = floor((trialNum - 1) / 8)) %>%
  group_by(gameid, condition, repNum) %>%
  summarize(numUniqueWordsUsed = length(unique(wordID))) %>%
  lmer(numUniqueWordsUsed ~ scale(repNum, scale=F) * condition + (1 + repNum | gameid),
       data = .,
       contrasts = list(condition = contr.sum(2))) %>%
  summary()
```

## Reaction times

```{r}
d %>% 
  mutate(condition = ifelse(condition == 'intermediateOnly', 'pure intermediate',
                          'pure subordinate')) %>%
  group_by(trialNum, condition) %>%
  summarize(RT = mean(timeFromRoundStart)) %>%
  ggplot(aes(x = trialNum, y = RT/1000, color = condition)) +
    geom_point(alpha = 0.2) +
    theme_few() + 
    guides(color = FALSE) +
    scale_color_colorblind() +
    geom_smooth(method = 'loess', span = 0.4) +
    ylim(0, NA) +
    ylab("reaction time (seconds)")
ggsave('../writing/cogsci18/figures/RTByCondiiton.pdf', width = 5, height = 4)
```

```{r}
summary(lmer(timeFromRoundStart ~ trialNum + (1 + trialNum| gameid),  
             data = d %>% 
               ungroup() %>% 
               mutate(timeFromRoundStart = log(timeFromRoundStart/1000), 
                      trialNum = scale(trialNum, center=F,scale= T))))
```

## Additional exploratory analyses

### *Individual* cumulative accuracy curves over time

Here we see very clearly the different pairs separate out (some never converge)

```{r}
ggplot(d, aes(x = trialNum, y = cumAcc, group = gameid)) +
  geom_line() +
  theme_few() + 
  guides(color = FALSE) +
  ylab("cumulative accuracy")
```

### Accuracy distributions by quartile of game

So we can clearly see the distributions... 

```{r}
d %>% 
  group_by(gameid, quarter) %>%
  summarize(percentCorrect = mean(acc)) %>%
  ggplot(aes(x = percentCorrect)) +
    geom_histogram(bins = 10) +
    theme_few() + 
    guides(color = FALSE) +
    facet_wrap(~ quarter) 
```

We see a slightly bimodal distribution where some people never converge (we'll exclude these for lexicon analyses).

# Post-test results

```{r}
postTest_word = read_delim('../data/experiment2/allWordPostTest.csv', '\t') %>%
  gather(object, meaning, blueSquare1:stripedCircle2) %>%
  mutate(blue = grepl('blue', object),
         red = grepl('red', object),
         striped = grepl('striped', object),
         spotted = grepl('spotted', object),
         circle = grepl("Circle", object),
         square = grepl("Square", object)) %>%
  right_join(d) %>%
  select(iterationName:square, condition) %>%
  group_by_at(vars(-condition)) %>%
  summarize(condition = first(condition)) %>%
  rename(text = target) %>%
  left_join(masterWordIDLookup) %>%
  left_join(masterGameIDLookup) 
length(unique(postTest_word$gameid))
```

## Consistency across two post-tests?

Read in individually because headers are all unique

```{r results="hide"}
file_list <- list.files('../data/experiment2/postTest_object/')
postTest_obj = data.frame()
for(file in file_list) {
  result <- read_delim(file = paste0('../data/experiment2/postTest_object/', file), delim = '\t') %>%
    gather(word, meaning, -iterationName, -gameid, -time, -target, -finalRole, -eventType)
  postTest_obj = rbind(postTest_obj, result)
}
```

Combine post-tests; take intersection of meanings as the best estimate of true meaning (more conservative)

```{r}
postTest_raw <- postTest_obj %>% 
  rename(object = target, text = word, objectToWordMeaning = meaning) %>%
  left_join(masterWordIDLookup) %>%
  inner_join(postTest_word %>% rename(wordToObjectMeaning = meaning), by = c('gameid', 'object', 'finalRole', 'wordID')) %>%
  select(-ends_with('.x'), -ends_with('.y')) %>%
  mutate(internalConsistency = objectToWordMeaning == wordToObjectMeaning) %>%
  mutate(meaning = objectToWordMeaning & wordToObjectMeaning)
```

Look at internal consistency

```{r}
cat('have both post-test measures for', 
    length(unique(paste0(postTest_raw$gameid, postTest_raw$finalRole))),
    'participants')

cat('average number of mismatches is ', median((postTest_raw %>%
  group_by(gameid, finalRole) %>%
  summarize(pctConsistent = 128-sum(internalConsistency)) %>% 
  ungroup())$pctConsistent))

postTest_raw %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(pctConsistent = 128- sum(internalConsistency)) %>%
  group_by(condition) %>%
  summarize(m = median(pctConsistent))

postTest_raw %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(pctConsistent = 128- sum(internalConsistency)) %>%
  ggplot(aes(x = pctConsistent)) +
    geom_histogram(bins = 35) +
    theme_few() +
    facet_wrap(~ condition) +
    xlab('% of mismatches among post-test responses')
```

## Consistency across partners

How often do players align on meanings?

We look at total overlap of matrix (i.e. how many cells differ). Compare the different measurements of meanings. 

```{r}
mismatches <- postTest_raw %>%
  select(-blue, -red, -striped, -spotted, -circle, -square, -internalConsistency) %>%
  gather(meaningType, value, meaning, objectToWordMeaning, wordToObjectMeaning) %>%
  spread(finalRole, value) %>%
  group_by(gameid, object, wordID, condition, meaningType) %>%
  summarize(match = listener == speaker) %>%
  group_by(gameid, condition, meaningType) %>%
  summarize(numMismatches = 128-sum(match))

missingPostTests <- unique((mismatches %>% filter(is.na(numMismatches)))$gameid)
cat('have both post-test measures for', 
    length(unique((mismatches %>% filter(!(gameid %in% missingPostTests)))$gameid)),
    'pairs')

ggplot(mismatches, aes(x = numMismatches)) +
    geom_histogram(binwidth = 1) +
    #geom_vline(aes(xintercept = mean(numMatching))) +
    #xlim(-0.1,1.1) + 
    theme_few() +
    xlab('# mismatches') +
   facet_wrap(meaningType ~ condition)
```

But note that pairs that didn't technically align that well on the post-test could still perform pretty well if one partner simply has a stricter meaning than the other but the difference is never relevant.

```{r}
mismatches <- postTest_raw %>%
  select(-blue, -red, -striped, -spotted, -circle, -square, -internalConsistency) %>%
  gather(meaningType, value, meaning, objectToWordMeaning, wordToObjectMeaning) %>%
  spread(finalRole, value) %>%
  group_by(gameid, object, wordID, condition, meaningType) %>%
  summarize(match = listener == speaker) %>%
  group_by(gameid, condition, meaningType) %>%
  summarize(numMismatches = 128-sum(match)) %>% 
  filter(!(gameid %in% missingPostTests))

mismatches %>% ungroup() %>% filter(meaningType == 'meaning') %>% summarize(m = median(numMismatches))
mismatches %>% group_by(condition) %>% filter(meaningType == 'meaning') %>% summarize(m = median(numMismatches))
summary(lm(numMismatches ~ condition, data = mismatches %>% ungroup() %>%filter(meaningType == 'meaning')))
```

## Do pairs with more similar lexica perform better?

```{r}
mismatchVsAcc <- mismatches %>% 
  inner_join(d) %>%
  group_by(gameid, condition) %>%
  summarize(acc = mean(overallAcc), numMismatches = mean(numMismatches)) %>%
  filter(!is.na(numMismatches))

ggplot(mismatchVsAcc, aes(x = acc, y = numMismatches, color = condition)) +
    geom_point() +
    geom_smooth(method = 'lm') +
    theme_few()

cor(mismatchVsAcc$numMismatches,mismatchVsAcc$acc, method = 'spearman')
```

## Any violations of contrast, or things that are described by more than one word?

Basically, only this team?

```{r}
'0888-836cf6dd-4836-4d3e-bc34-2ad06f1a5352'
```

## Main results

We remove nonConvergers for subsequent analyses

```{r}
postTest_clean <- postTest_raw %>%
  filter(!(gameid %in% nonConvergers)) 
print(length(unique(postTest_clean$gameid)))
```

### Check whether cleaning out non-covergers basically equates internal consistency across conditions

```{r}
postTest_clean %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(pctConsistent = 128- sum(internalConsistency)) %>%
  group_by(condition) %>%
  summarize(m = mean(pctConsistent))
```

### Result 1: Vocab size by condition.

```{r}
lexiconSize <- postTest_clean %>%
  group_by(gameid, finalRole, wordID, condition) %>%
  summarize(numObjects = sum(meaning)) %>%
  filter(numObjects > 0) %>%
  group_by(gameid, finalRole, condition) %>%
  tally() %>%
  group_by(condition, gameid) %>%
  summarize(vocabSize = median(n, na.rm = T)) 

summary(lm(vocabSize ~ condition, data = lexiconSize))
```

### Coverage in shared lexicon? 

This is pretty conservative, since it uses the 'intersection' metric of internal consistency: a word is only in a particular player's lexicon if they marked it in both directions, hence we're probably under-estimating their vocab. If we've underestimated both peoples' vocabs, we've also underestimated their overlap, which is probably dragging these down. Still, we get a median coverage of 7 words... 

```{r}
 coverageDF <- postTest_clean %>%
  select(-blue, -red, -striped, -spotted, -circle, -square, -internalConsistency) %>%
  filter(!(gameid %in% missingPostTests)) %>%
  gather(meaningType, value, meaning, objectToWordMeaning, wordToObjectMeaning) %>%
  spread(finalRole, value) %>%
  group_by(gameid, object, wordID, condition, meaningType) %>%
  summarize(match = listener & speaker) %>%
  filter(meaningType == 'meaning') %>%
  group_by(gameid, object, condition) %>%
  summarize(numWord = sum(match)) %>%
  group_by(gameid, condition) %>%
  summarize(numObjectsWithSingleWord = sum(numWord == 1),
            numObjectsWithMultipleWords = sum(numWord > 1))


coverageDF %>% group_by(condition) %>% summarize(m1 = mean(numObjectsWithSingleWord), mMore = mean(numObjectsWithMultipleWords))

ggplot(coverageDF, aes(x = condition, y = numObjectsWithSingleWord)) +
    geom_violin() +
    ylab('# objects with shared words') +
    theme_few()
```

### Result 2: How many abstract vs. specific terms overall?

```{r}
lexiconCounts <- postTest_clean %>% 
  group_by(gameid, finalRole, wordID, condition) %>%
  summarize(numMeanings = sum(meaning)) %>%
  group_by(condition, numMeanings) %>%
  tally() %>%
  group_by(condition) %>%
  mutate(pct = n/sum(n), ci = 1.96*sqrt(pct*(1-pct)/sum(n))) 

ggplot(lexiconCounts, aes(x = condition, fill = factor(numMeanings), y = pct)) +
    geom_bar(stat = 'identity', position = position_fill(reverse = FALSE), width=.5) +
    #geom_errorbar(aes(ymax = pct + ci, ymin = pct - ci), data = lexiconCounts %>% filter(numMeanings < 3), width = 0, stat = 'identity', position = 'identity') +
    #facet_wrap(~ condition) +
    theme_few() +
    xlab("# objects words refer to") +
      guides(fill=FALSE)

ggsave('../writing/cogsci18/figures/lexiconContent.pdf', width = 3, height = 3)
```

### Proportion of specific & abstract within single lexicon?

```{r}
postTest_clean %>% 
  group_by(gameid, finalRole, condition, wordID) %>%
  filter(meaning) %>%
  summarize(specific = sum(meaning) == 1,
            abstract = sum(meaning) > 1) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSpecific = sum(specific),
            numAbstract = sum(abstract)) %>%
  mutate(condition = ifelse(condition == 'intermediateOnly', 'coarse', 'fine')) %>%
  mutate(condition = factor(condition, levels = c('coarse', 'fine'))) %>%
  ggplot(aes(x = numSpecific, y = numAbstract)) +#, color = numSub > 0 & numBasic > 0)) +
    geom_hex(binwidth = c(2,1))  +
    facet_grid(~ condition) +
    theme_few() +
    xlab("# words referring to single object") +
    ylab("# words referring \n to multiple objects") +
    theme(aspect.ratio=1, legend.position = 'top') +
    ylim(NA, 6) +
    scale_x_continuous(breaks=c(0,2, 4, 6, 8, 10), limits = c(NA,11)) +
    scale_fill_gradient(low = "grey90", high = "black")  +
    #ylim(0,6)
    guides(fill=FALSE)
  
ggsave("../writing/cogsci18/figures/fullLexiconReport.pdf", width = 6, height =3)
```

What is modal response in each condition?

```{r}
postTest_clean %>% 
  group_by(gameid, finalRole, wordID) %>%
  filter(meaning == 1) %>%
  summarize(subordinate = sum(meaning) == 1,
            basic = (sum(meaning) == 2 & 
                       (all(red) | all(blue) | all(striped) | all(spotted)))) %>%
  group_by(gameid, finalRole) %>%
  summarize(numSub = sum(subordinate),
            numBasic = sum(basic)) %>%
  left_join(d) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(numSub = mean(numSub), numBasic=mean(numBasic)) %>%
  group_by(condition, numSub, numBasic) %>%
  tally() %>%
  group_by(condition) %>%
  mutate(pct = n/sum(n)) %>%
  select(-n) %>%
  filter(pct == max(pct))
```

## Proportion of specific vs. abstract distribution within lexicon

How many objects does each label correspond to (i.e. how many meanings at sub-level vs. basic-level)

```{r}
pctDF <- postTest_clean %>%
  group_by(gameid, finalRole, wordID, condition) %>%
  filter(meaning == 1) %>%
  summarize(specific = sum(meaning) == 1,
            abstract = sum(meaning) > 1) %>%
  group_by(gameid, finalRole, condition) %>%
  summarize(pctSpecific = sum(specific)/(sum(specific) + sum(abstract)),
            pctAbstract = sum(abstract)/(sum(specific) + sum(abstract))) %>% 
  group_by(condition) %>%
  summarize(m = mean(pctSpecific))

summary(lm(pctAbstract ~ condition, data = pctDF))
```
