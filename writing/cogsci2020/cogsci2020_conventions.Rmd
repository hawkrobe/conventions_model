---
title: |
    | Generalizing meanings from partners to populations:
    | Hierarchical inference supports convention formation on networks
    
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Morton Ann Gernsbacher (MAG@Macc.Wisc.Edu)} \\ Department of Psychology, 1202 W. Johnson Street \\ Madison, WI 53706 USA
    \AND {\large \bf Sharon J.~Derry (SDJ@Macc.Wisc.Edu)} \\ Department of Educational Psychology, 1025 W. Johnson Street \\ Madison, WI 53706 USA}

abstract: >
   A key property of linguistic conventions is that they hold over an entire community of speakers, allowing us to communicate efficiently even with people we have never met before. 
   At the same time, much of our language use is partner-specific: we know that words may be understood differently by different people based on local common ground.
   This poses a challenge for accounts of convention formation. 
   Exactly how do agents make the inferential leap to community-wide expectations while maintaining partner-specific knowledge?
   We propose a hierarchical Bayesian model of convention to explain how speakers and listeners abstract away meanings that seem to be shared across partners. 
   \aeg{changed convention explaining just above to convention to explain, to avoid garden path}
   To evalute our model's predictions, we then conducted an experiment where participants played an extended natural-language communication game with different partners in a small community. 
   We examine several measures of generalization across partners, and find a combination of local adaptation and global convergence.
   These results suggest that local partner-specific learning is not only compatible with global convention formation but may facilitate it when coupled with a powerful hierachical inductive mechanism. 
\aeg{great!}
    
keywords: >
   convention; generalization; 
    
output: cogsci2016::cogsci_paper 
#final-submission: \cogscifinalcopy
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, 
                      fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r, libraries}
library(grid)
library(tidyverse)
library(tidyboot)
library(broom.mixed)
library(ggthemes)
library(xtable)

clicks <- read_csv('../../data/clicks.csv')
messages <- read_csv('../../data/messages.csv')

completeNetworks <- clicks %>% 
  distinct() %>% 
  group_by(networkid) %>% 
  tally() %>%
  filter(n == 96) %>% 
  pull(networkid)

numParticipantsRecruited <- length(unique(clicks$participantid))
numNetworks <- length(completeNetworks)

relevantMessages <- messages %>% 
  filter(networkid %in% completeNetworks) %>%
  filter(role == "speaker") %>%
  group_by_at(vars(-content)) %>%
  summarize(content = first(content)) %>%
  mutate(uttLength = str_count(content, " ") +1,
         repnum = floor(trialnum / 4)) %>%
  rowwise() %>%
  mutate(repnum = repnum + 1) %>%
  group_by(participantid, partnernum) %>%
  mutate(ordinalrep = ifelse(repnum == min(repnum), 'first', 'second')) %>%
  group_by(participantid, trialnum, networkid, roomid, target, partnernum, repnum, ordinalrep) %>%
  summarize(m = sum(uttLength)) %>%
  ungroup()

relevantClicks <- clicks %>%
  filter(networkid %in% completeNetworks) %>%
  mutate(repnum = floor(trialnum / 4)) %>%
  mutate(correct = object_id == 'target') %>%
  left_join(relevantMessages %>% select(target, trialnum, networkid, roomid, ordinalrep) %>% distinct(), by = c( 'roomid', 'trialnum', 'networkid'))
```

To communicate successfully, speakers and listeners must share a common system of semantic meaning in the language they are using. 
These meanings are *conventional* in the sense that they are sustained by the expectations each person has about others [@lewis_convention:_1969; @bicchieri_grammar_2006]. 
A key property of linguistic conventions is that they hold over an entire community of speakers, allowing us to communicate efficiently even with people we've never met before. 
But exactly how do we make the inferential leap to community-wide expectations from our experiences with specific partners? 
Grounding collective convention formation in individual cognition requires an explicit *theory of generalization* capturing how people transfer what they have learned from one partner to the next.

One influential theory is that speakers simply ignore the identity of different partners and update a single monolithic representation after every interaction [@steels_self-organizing_1995; @barr_establishing_2004; @young_evolution_2015; @baronchelli_emergence_2018]. 
We call this a *complete-pooling* theory because data from each partner is collapsed into an undifferentiated pool of evidence [@gelman2006data]. 
Complete-pooling models have been remarkably successful at predicting collective behavior on networks, but have typically been evaluated only in settings where anonymity is enforced. 
For example, @centola_spontaneous_2015 asked how large networks of participants coordinated on conventional names for novel faces.
On each trial, participants were paired with a random neighbor but were not informed of that neighbor's identity, or even the total number of different possible neighbors. 

While complete-pooling may be appropriate for some everyday social interactions, such as coordinating with anonymous drivers on the highway, it is less tenable for everyday communicative settings. 
Knowledge about a partner's identity is both available and relevant for conversation [@eckert_three_2012].
Extensive evidence from psycholinguistics has demonstrated the *partner-specificity* of our language use [@clark_using_1996]. 
Because meaning is grounded in the evolving 'common ground' shared with each partner, meanings established over a history of interaction with one partner are not necessarily transfered to other partners [@wilkes-gibbs_coordinating_1992; @metzing_when_2003]. 
Partner-specificity thus poses clear problems for complete-pooling theories but can be easily explained by another simple model, where agents maintain separate expectations about meaning for each partner. 
We call this a *no-pooling* model. 
The problem with no-pooling, of course, is that agents are forced to start from scratch with each partner.
Community-level expectations never get off the ground. 

What theory of generalization, then, can explain partner-specific meaning but also allow conventions to spread through communities? 
We propose a *partial-pooling* account that offers a compromise between these extremes. 
Unlike complete-pooling and no-pooling models, we propose that beliefs about meaning have hierarchical structure. 
That is, the meanings used by different partners are expected to be drawn from a shared community-wide distribution but are also allowed to differ from one another in systematic, partner-specific ways. 
This structure provides an inductive pathway for abstract population-level expectations to be distilled from partner-specific experience [see also @KleinschmidtJaeger15_RobustSpeechPerception; @tenenbaum_how_2011]. 


We begin by formalizing this account in a probabilistic model of communication and presenting several simulations of listener and speaker behavior within and across partners. 
Next, we test the qualitative predictions of this model in a behavioral experiment.
Participants were paired for a series of extended reference games with each neighbor in small networks. 
Our results showed signatures of *ad hoc* convention formation within dyads, but also gradual generalization of these local pacts across subsequent partners as the network converged. 
Taken together, these results suggest that local partner-specific learning is not only compatible with global convention formation but may facilitate it when coupled with a powerful hierachical inductive mechanism. 

# A hierarchical Bayesian model of convention

```{r model_schematic, fig.env = "figure", fig.pos = "t", out.width = "225px", fig.height=1, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:task1model} Schematic of hierachical Bayesian model."}
knitr::include_graphics("figs/task1_model.pdf")
```

In this section, we provide an explicit computational account of the cognitive mechanisms supporting the balance between community-level stability and partner-specific flexibility.
Specifically, we show how the dyadic convention formation model of @hawkins_convention-formation_2017 can be extended with a principled mechanism for generalization across multiple partners.
This model begins with the idea that knowledge about meanings can be represented probabilistically: agents have uncertainty about what lexical meaning their current partner is using [@bergen_pragmatic_2016]. 
In our hierarchical model, this lexical uncertainty is represented by a multi-level prior. 

At the highest level of the hierarchy is *community-level* uncertainty $P(\Theta)$, where $\Theta$ represents an abstract "overhypothesis" about the overall distribution of possible partners. 
$\Theta$ then parameterizes the agent's *partner-specific* uncertainty $P(\phi_{k} | \Theta)$, where $\phi_k$ represents the specific system of meanings used by partner $k$ (see Fig. \ref{fig:task1model}). 
\aeg{Is the uncertainty about the system of {\bf meanings} or about "meaning representations" or form-meaning correspondences (aka constructions)}?
Given observations $D_k$ from repeated communicative interactions with $k$, the agent updates their beliefs about the latent system of meaning using Bayes rule:
\begin{equation}
\begin{array}{rcl}
P(\phi_k, \Theta | D_k)  & \propto &  P(D_k | \phi_k, \Theta) P(\phi_k, \Theta) \\
                           & =   & P(D_k | \phi_k) P(\phi_k | \Theta) P(\Theta)
\end{array}
\end{equation}
\aeg{The figure is explained below, but maybe make the individual-images be made slightly smaller so that it's clear they belong to the second row?  Also, the 2nd row label on the left also seems to imply any novel partner, but then it's odd to assign a random partner two distinct colors. Isn't it "lexical prior for individuals?"?
This joint inference decomposes the problem of partner-specific learning into two terms, a prior term $P(\phi_k | \Theta)P(\Theta)$ and a likelihood term $P(D_k | \phi_k)$.
The prior captures the idea that different partners share some aspects of meaning in common.
In the absence of strong information about partner-specific language use departing from this common structure, the agent ought to be regularized toward generalizable knowledge of their community's conventions [@davidson_nice_1986].
The likelihood represents predictions about how a partner using a particular system of meaning will use language.
\aeg{just to think about: I agree but doesn't this tuck part of the solution into the prior? Does this then raise the question as to whether the bias to generalize is learned and how.  It certainly could be learned in a more bottom-up way, as a preference for simplicity or just by witnessing multiple partners using shared conventions.}

This joint posterior over meanings has two consequences for convention formation.
First, it allows agents to maintain partner-specific expectations $\phi_k$ by marginalizing over community-level uncertainty:
\begin{equation}
P(\phi_k | D_k) = \int_{\Theta}P(D_k | \phi_k) P(\phi_k | \Theta) P(\Theta)  d\Theta
\end{equation}
Second, the hierarchical structure provides an inductive pathway for data to inform beliefs about community-wide conventions.
Agents update their beliefs about $\Theta$ by marginalizing over data accumulated from different partners:
\begin{equation}
P(\Theta | D) = P(\Theta) \int_{\phi} P(D_k | \phi_k) P(\phi_k | \Theta) d\phi
\end{equation}
where $D = \bigcup_{k=1}^N D_k$, $\phi = \phi_1 \times \dots \times \phi_N$, and $N$ is the number of partners previous encountered. 

After multiple partners are inferred to have a similar system of meaning, beliefs about $\Theta$ shift to represent this abstracted knowledge: it becomes more likely that a novel partner will share it as well.
This transfer is sometimes referred to as a "sharing of strength" or "partial pooling" [@gelman2006data] because pooled data is smoothly integrated with domain-specific detail depending on the data available.

## Model simulations

```{r cache=T}
unfinished <- read_csv('../../simulations/hierarchical/compiled_network_output.csv') %>%
  group_by(chainNum) %>%
  tally() %>%
  filter(n < 24) %>%
  pull(chainNum)

modelNetworks <- read_csv('../../simulations/hierarchical/compiled_network_output.csv') %>%
  filter(!(chainNum %in% unfinished)) %>%
  mutate(row_number = row_number() - 1,
       chainNum = floor(row_number / 24),
       repNum = row_number %% 4,
       correct = objectPicked == 'object1') 

modelconvergence.toPlot <- modelNetworks %>%
  filter(!(chainNum %in% unfinished)) %>%
  group_by(chainNum, speakerID, partnerID) %>%
  mutate(ordinalRep = ifelse(time == first(time), 'first', 'second')) %>%
  select(ordinalRep, speakerID,  partnerID, utt, chainNum) %>%
  group_by(chainNum, ordinalRep, partnerID) %>%
  tidybayes::gather_pairs(speakerID, utt, row = 'speaker1', col = 'speaker2', x = 'utt1', y = 'utt2') %>%
  mutate(partnerType = case_when(partnerID == 1 ~ ifelse((speaker1 == 2 && speaker2 == 1) ||
                                                            (speaker1 == 4 && speaker2 == 3),
                                                          'within', 'between'),
                                 partnerID == 2 ~ ifelse((speaker1 == 3 && speaker2 == 1) ||
                                                            (speaker1 == 4 && speaker2 == 2),
                                                          'within', 'between'),
                                 partnerID == 3 ~ ifelse((speaker1 ==4 && speaker2 == 1) ||
                                                            (speaker1 == 3 && speaker2 == 2),
                                                          'within', 'between')),
         match = utt1 == utt2) %>%
  filter(utt1 %in% c('word1', 'word2') && utt2 %in% c('word1', 'word2')) %>%
  group_by( partnerID, partnerType) %>%
  tidyboot_mean(match) 
```

```{r model_results, cache = T, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:simulations} Model predictions across a series of different partners."}
library(grid)
library(gridExtra)

model.plt1 <- read_csv('../../simulations/hierarchical/listenerOutput.csv') %>%
  ggplot(aes(x = time, y = prediction, color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    geom_hline(yintercept = .5) +
    theme_few() +
    xlab('time') +
    ylab("accuracy") +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11,13,15)) +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    guides(color = FALSE) +
    labs(tag = "A", title = "Listener") +
    theme(aspect.ratio = 4/5)

model.plt2 = read_csv('../../simulations/hierarchical/speakerOutput_reps.csv') %>%
  mutate(expectedNumWords = 2 * exp(prediction) + 1 * (1 - exp(prediction))) %>%
  group_by(time, partnerID) %>%
  tidyboot_mean(expectedNumWords) %>%
  ggplot(aes(x = time, y = empirical_stat, fill = factor(partnerID), color = factor(partnerID), group = factor(partnerID))) +
    geom_point() +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.3, color=NA) +
    theme_few() +
    ylim(1, 2) +
    geom_hline(yintercept = 1) +
    xlab('time') +
    ylab("# words") +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11,13,15)) +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    guides(color = FALSE, fill = FALSE) +
    labs(tag = "B", title = "Speaker") +
    theme(aspect.ratio = 4/5)

model.plt3 <- modelconvergence.toPlot %>%
  ungroup() %>%
  mutate(comparison = ifelse(partnerType == 'within', 'within dyad', 'across dyads')) %>%
  ggplot(aes(x = partnerID, y = empirical_stat, color = comparison, group = comparison)) +
    geom_point() +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    labs(tag = "C", title = "Network") +
    theme_few() +
    ylab("alignment") +
    xlab("partner #") +
    ylim(0, 1) +
    scale_x_continuous(breaks = c(1,2,3)) +
    scale_color_manual(values = c('#000000', RColorBrewer::brewer.pal(5,'Blues')[5])) +
    annotate(geom="text", x=1, hjust = 0, y=.9, label="within dyads", color="#08519C") +
    annotate(geom="text", x=1, hjust = 0, y=.4, label="across dyads", color="black") +
    guides(color = FALSE) +
    theme(aspect.ratio = 4/5)

#ggsave('grid_search.pdf', height = 10, width = 10, unit = 'in')
gridExtra::grid.arrange(model.plt1, model.plt2, model.plt3, ncol = 3)
```

We investigate the qualitative predictions of this model under three increasingly complex scenarios. 
In all of these scenarios, speaker and listener agents play a reference game with a set of two objects $\{o_1, o_2\}$.
On each trial, one of these objects is designated for the speaker as the *target*. 
They must select from a set of utterance $\{u_0, \dots, u_j\}$ to convey the identity of the target to the listener.
Upon hearing this utterance, the listener selects which of the objects they believe to be the target and then receives feedback about the true target.
The resulting data $D_k$ from an interaction with partner $k$ thus consists of utterance-object pairs $\{(u, o)_t\}$ for each trial $t$, as well as information about the context of objects.

Given this reference game setting, we can now explicitly specify the likelihood and prior terms. 
We consider a likelihood given by the Rational Speech Act (RSA) framework, which formalizes the Gricean assumption of cooperativity [@GoodmanFrank16_RSATiCS;@FrankeJager16_ProbabilisticPragmatics].
A pragmatic speaker $S_1$ attempts to trade off informativity against the cost of producing an utterance, while a pragmatic listener $L_1$ inverts their model of the speaker to infer the intended target.
The chain of recursive social reasoning grounds out in a \emph{literal listener} $L_0$, who identifies an intended meaning using their knowledge of lexical items $\mathcal{L}_{\phi_k}$. 
This model can be formally specified as follows:
$$
\begin{array}{rcl}
L_0(o | u, \phi_k) &\propto  & \exp\{\mathcal{L}_{\phi_k}(u,o)\} \\
S_1(u | o, \phi_k) &\propto &  \exp\{w_I \cdot \log L_0(o | u, \phi_k) - w_C \cdot \textrm{cost}(u)\}   \\
L_1(o | u, \phi_k) &\propto  & S_1(u | o, \phi_k) P(o) 
\end{array}
$$
where $w_I$ and $w_C$ are free parameters controlling the relative weights on the informativity and parsimony, respectively^[Throughout our simulations, we set $w_I = 11,~w_C = 7$. A grid search over parameter space revealed different behaviors in different regimes (e.g. when cost was weighted too strongly, multi-word utterances were not produced); we leave broader exploration of this space to future work.]. 
We define $P(D_k | \phi_k)$ as the probability of the data under a pragmatic listener $L_1$.
We also use this RSA model to simulate the behavior of uncertain speakers $S$ and listeners $L$. 
Utterances and object selections are sampled from the posterior predictive, marginalizing over lexical uncertainty.

Finally, we must specify the form of the lexical prior and a method to perform inference in this model.
We assume $\Theta$ is a matrix with an entry for each utterance-object pair $(u_i, o_j)$, and use independent Gaussian distributions for each $\Theta_{ij} \in \Theta$ as a hyper-prior.
We then centered our partner-specific prior $\phi_{ij} \in \phi$ at the shared value for a particular partner:
$$\begin{array}{rcl}
P(\Theta_{ij}) & \sim & \mathcal{N}(0, 1)\\
P(\phi_{ij} | \Theta_{ij}) & \sim & \mathcal{N}(\Theta_{ij}, 1)
\end{array}$$
The variances chosen in these priors represent assumptions about how far partner-specific priors can drift from the community-wide value.

For all simulations, we used the implementation of variational inference in WebPPL [@GoodmanStuhlmuller14_DIPPL]. 
Variational methods transform probabilistic inference problems into optimization problems by approximating the true posterior with a parameterized family.
Specifically, we make a \emph{mean-field} approximation and assume that the full posterior can be factored into independent Gaussians for each random variable. 
We then optimize the parameters of these posterior Gaussians by minimizing the evidence lower bound (ELBO) objective [see @murphy2012machine for more details].
We run 50,000 steps of gradient descent on the first observation to obtain a posterior, compute the agent's marginal prediction for the next observation by taking the expectation over 50,000 samples from the posterior predictive, then continue running gradient descent on the same parameters after adding the new observation in the data.

### Simulation 1: Listener accuracy across partners

The key predictions of our model concern the pattern of generalization across partners.
In our first simulation, we consider the partner-specificity of a *listener*'s expectations about which object is being referred to.
To observe the model's behavior in the simplest case, we assume the speaker has a vocabulary of two single-word utterances $\{u_1, u_2\}$ with equal cost, and feed the listener the same utterance and feedback about the target object ($\{o_1, u_1\}$) on every trial.
Instead of presenting these observations from a single partner, or randomly choosing a different partner on every trial, we swap in a new partner every block of 4 trials.

Our results are shown in Fig. \ref{fig:simulations}A.
The listener begins at chance due to its uninformative prior, but after observing several trials of evidence from the same partner, it rapidly infers the meaning of $u_1$ and learns to choose the true target with high accuracy.
When a second partner is introduced, however, it reverts nearly to its original state.
This reversion is due to ambiguity about whether the behavior of the first partner was idiosyncratic or attributable to community-level conventions.
In the absence of data from other partners, this data is more parsimoniously explained with a partner-specific model.
After observing multiple partners behave similarly, however, we find that this knowledge has gradually been incorporated into community-level expectations. 
This is evident in the much stronger initial expectations about meaning by its fourth partner ($\sim$ 75\% accuracy vs. 50\% with the first partner.)

```{r task_display, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "\\label{fig:task1_display} Experimental design. (A) Participants were placed in fully-connected networks of 4 and (B) played repeated reference games with each partner."}
knitr::include_graphics("figs/design.pdf")
```

### Simulation 2: Speaker utterance length across partners

Next, we examined our model's predictions about how a *speaker*'s referring expressions will change with successive listeners.
While it has been frequently observed that messages reduce in length across repetitions with a single partner [@krauss_changes_1964], and sharply revert back to longer utterances when a new partner is introduced [@wilkes-gibbs_coordinating_1992], the key prediction distinguishing our model concerns behavior across subsequent partner boundaries.
Complete-pooling accounts predict no change in number of words when a new partner is introduced.
No-pooling accounts predict that roughly the same initial description length will re-occur with every subsequent interlocutor. 
Here we show that a partial pooling account predicts a more complex pattern of generalization.

To allow for reduction, we allowed a set of four primitive utterances, $\{u_1, u_2, u_3, u_4\}$, to be combined into two-word conjunctions, e.g. $\{u_1+u_2, u_3+u_4\}$.
The meanings of these two-word utterances were determined compositionally from the values of the primitive utterances^[We used the standard product T-norm semantics for conjunction in fuzzy logic, where values lie in the $[0,1]$ interval. Because we used a Gaussian prior with support over the real numbers, we first used a logistic function to map primitive values to the unit interval, and a logit function to map the product back to the original domain.].
Additionally, we placed a weakly biased initial prior over $\Theta$: two of the utterances ($u_1$ and $u_2$) were assumed to apply more strongly to $o_1$ and the other two ($u_3$ and $u_4$) more strongly to $o_2$.
This weak prior led the speaker to prefer conjunctions at the outset and thus allowed us to examine the speaker's shifting preference for conjunctions.
\aeg{Just a thought: this sounds kind of klugy to me.  Is there a way to better motivate it? Maybe, since there were 4  us and 2 objects, priors were initialized pseudo-randomly so that 2 us's were biased toward o1 and 2 toward o2? Or maybe its motivated by the idea that the default assumption is that all given options are relevant to the task in some way? } 

To focus on the speaker's behavior, we paired it with a fixed listener who always correctly selected the target, and ran 48 independent simulations. 
First, we find that successful interactions with the first partner become more efficient over trials as the model learns that the shorter utterances will be meaningful to their partner [e.g. the speaker begins to prefer either $u_1$ or $u_2$ to refer to $o_1$ instead of the conjunction; see @hawkins_convention-formation_2017 for further interpretation of this effect].
\aeg{I feel like this doesn't capture *why* this happens: if the model is successful, why doesn't it stick with what works? Is there a prior to be efficient? or does the second conjunct introduce possible noise or interference?}
Second, we find that speakers revert back to a longer description at the first partner swap, just as our listener model did: evidence from a single partner is relatively uninformative about the community-level distribution.
\aeg{the speaker-agents rather than speakers? also, how does a model become more confident? Just ignore if these are standard modeling-community conventions. : )}
After interacting with several partners, however, speakers become increasingly confident that one or the other of the short labels is shared across the entire community, and are less likely to begin an interaction with a long utterance (Fig. \ref{fig:simulations}B).

### Simulation 3: Network convergence

The first two simulations presented a single adaptive agent with a fixed partner to understand its gradient of generalization. 
In our final simulation, we test the consequences of the proposed hierarchical inference scheme for a network of interacting agents.
From each individual agent's perspective, this simulation is identical to the earlier ones (i.e. a sequence of 3 different partners).
Because all agents are simultaneously making inferences about the others, however, the network as a whole faces a coordination problem.
For example, in the first block, agents 1 and 2 may coordinate on using $u_1$ to refer to $o_1$ while agent 3 and 4 coordinate on using $u_2$. 
Once they swap partners, they must negotiate this potential mismatch in usage. 
How does the network as a whole manage to coordinate?
\aeg{nicely described!}

We used a round-robin scheme to schedule four agents into three blocks of interaction, with each agent taking turns in the speaker and listener roles, and again simulated 48 independent networks.
We measured alignment by computing whether different pairs of agents produced the same one-word utterances as speakers.
We compared the alignment between currently interacting agents (i.e. *within* a dyad) to the alignment between agents in the network who were not interacting (i.e. *across* dyads).
During interaction with the first partner, alignment across dyads was roughly at chance, reflecting the arbitrariness of whether speakers reduce to $u_1$ or $u_2$. 
In the absence of hierarchical generalization, we would expect subsequent blocks to show similar chance levels, as partner-specific conventions would continually need to be re-negotiated from scratch.
Instead, we find that alignment across dyads gradually increases, suggesting that partial pooling across partners leads to emergent consensus (Fig. \ref{fig:simulations}C).

# Behavioral experiment

```{r reduction, cache=T, fig.env = "figure*", fig.pos = "t!", fig.width=7, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=1, fig.cap = "\\label{fig:results}(A)Increase in accuracy across partners, (B) reduction in number of words across partners, (C) network convergence."}
clicks.mean <- relevantClicks %>%  
  group_by(repnum, partnernum) %>%
  tidyboot::tidyboot_mean(correct)

results.plt1 <- clicks.mean %>%
  mutate(t = repnum + 4 * partnernum) %>%
  ggplot(aes(x = t, y = empirical_stat, fill = factor(partnernum), color = factor(partnernum), group = partnernum)) +
    geom_line() +
    geom_point() +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.2, color =NA) +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11)) +
    theme_few() +
    theme(aspect.ratio = 1) +
    guides(color = FALSE, fill = FALSE) +
    labs(x = 'time', y = 'accuracy', tag = 'A')

messages.mean <- relevantMessages %>%
  group_by(partnernum, repnum) %>%
  tidyboot::tidyboot_mean(column = m)

results.plt2 <- messages.mean %>%
  mutate(t = repnum + 4 * partnernum) %>%
  ggplot(aes(x = t, y = empirical_stat, fill = factor(partnernum), color = factor(partnernum), group = partnernum)) +
    geom_point() +
    geom_line() +
    scale_color_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_fill_manual(values = rev(RColorBrewer::brewer.pal(8,'Blues'))) +
    scale_x_continuous(breaks = c(1,3, 5,7,9,11)) +
    geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = .2, color =NA) +
    theme_few() +
    theme(aspect.ratio = 1) +
    guides(color = FALSE, fill = FALSE) +
    ylim(0,NA) +
    labs(x = "time", y = "mean # words", tag = 'B')

partnerLookup <- messages %>%
  group_by(networkid) %>%
  do(., mutate(., speakerID = group_indices(., participantid))) %>%
  group_by(networkid, roomid, partnernum, speakerID) %>%
  tally() %>%
  group_by(networkid, roomid) %>%
  mutate(partnerID = ifelse(speakerID == first(speakerID), last(speakerID), first(speakerID))) %>%
  ungroup() %>%
  mutate(speaker1 = as.factor(speakerID)) %>%
  select(-n, -roomid) 

stopwords_regex = paste(c('ive', tm::stopwords('SMART')), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')

matches <- messages %>% 
  filter(role == "speaker") %>%
  filter(networkid %in% completeNetworks) %>%
  group_by(participantid, partnernum) %>%
  mutate(ordinalrep = ifelse(repid == min(repid), 'first', 'second'),
         content = tolower(content),
         content = stringr::str_replace_all(content, stopwords_regex, ""),
         content = str_squish(gsub("[[:punct:]]", "", content))) %>%
  group_by(networkid) %>%
  do(., mutate(., speakerID = as.integer(group_indices(., participantid)))) %>%
  group_by(ordinalrep, networkid, partnernum, stimsetid, repid, target, participantid, speakerID, trialnum) %>%
  summarize(content = paste0(content, collapse = ' ')) %>%
  group_by(networkid, repid, target, partnernum, speakerID) %>%
  tidybayes::gather_pairs(speakerID, content, row = 'speaker1', col = 'speaker2', x = 'utt1', y = 'utt2') %>%
  left_join(partnerLookup) %>%
  rowwise() %>%
  mutate(partnerType = ifelse(speaker2 == partnerID, 'within', 'across'),
         matchRate = length(intersect(strsplit(utt1, " ")[[1]], strsplit(utt2, " ")[[1]])),
         utt1Length = length(strsplit(utt1, " ")[[1]]),
         utt2Length = length(strsplit(utt2, " ")[[1]])) %>%
  arrange(networkid, partnernum)

matches.summary <- matches %>% 
  group_by(partnernum, partnerType) %>%
  tidyboot_mean(matchRate> 0)

results.plt3 <- matches.summary %>%
  ungroup() %>%
  mutate(comparison = ifelse(partnerType == 'within', 'within dyad', 'across dyads')) %>%
  ggplot(aes(x = partnernum, y = empirical_stat, color = comparison, group = comparison)) +
    geom_line() +
    geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +
    labs(x = "partner #", y = "alignment", tag = 'C') +
    ggthemes::theme_few() +
    ylim(0, 1) +
    scale_color_colorblind() +
    scale_x_continuous(breaks = c(1,2,3)) +
    annotate(geom="text", x=0.5, hjust = 0, y=.9, label="within dyads", color="orange") +
    annotate(geom="text", x=0.5, hjust = 0, y=.2, label="across dyads", color="black") +
    guides(color = FALSE) +
    theme(aspect.ratio = 1) 

gridExtra::grid.arrange(results.plt1, results.plt2, results.plt3, ncol = 3)
```

To evaluate the qualitative predictions observed in our simulations, we designed a natural-language communication experiment following roughly the same network design.
Instead of anonymizing partners, as in many previous empirical studies of convention formation, we divided the experiment into blocks of extended dyadic interactions with stable, identifiable partners [see @fay_interactive_2010; @garrod_conversation_1994 for similar designs].
Each block was a full repeated reference game, where participants had to coordinate on an *ad hoc* convention, or *pact*, for how to refer to novel objects with their partner [@BrennanClark96_ConceptualPactsConversation].
As the simulations demonstrated, our model predicts that these pacts will reset at partner boundaries, but that agents should be increasingly willing to transfer expectations from one partner to another in their community. 

### Participants 

We recruited 92 participants from Amazon Mechanical Turk to play a series of interactive, natural-language reference games.

### Stimuli and procedure

Each participant was randomly assigned to one of `r numNetworks` fully-connected networks with three other participants as their 'neighbors' (Fig. \ref{fig:task1_display}A). 
Each network was then randomly assigned one of three distinct "contexts" containing abstract tangram stimuli taken from Clark and Wilkes-Gibbs [-@clark_referring_1986].
The experiment was structured into a series of three repeated reference games with different partners, using these same four stimuli as referents.
Partner pairings were determined by a round-robin schedule (Fig. \ref{fig:task1_display}B).
The trial sequence for each reference game was composed of four repetition blocks, where each target appeared once per block.
After completing sixteen trials with one partner, participants were introduced to their next partner and asked to play the game again. 
This process repeated until each participant had partnered with all three neighbors.
Because some pairs within the network took longer than others, we sent participants to a temporary waiting room if their next partner was not ready. 

Each trial proceeded as follows.
First, one of the four tangrams in the context was highlighted as the \emph{target object} for the "speaker." 
They were instructed to use a chatbox to communicate the identity of this object to their partner, the "listener" (see Fig. \ref{fig:task1_display}C).
The listener could reply freely through the chatbox but was asked to ultimately make a selection from the array. 
Finally, both participants in a pair were given full feedback on each trial about their partner's choice and received bonus payment for each correct response. 
The order of the stimuli on the screen was randomized on every trial to prevent the use of spatial cues (e.g. 'the one on the left').
The display also contained an avatar representing their current partner to emphasize that they were speaking to the same partner for an extended period.


```{r}
# testing raw 
mainEffect.acc <- relevantClicks %>%  
  lme4::glmer(correct ~ repnum + (1 + repnum | participantid) + (1 + repnum |target),
              family = 'binomial',
              control = lme4::glmerControl("bobyqa"),
              data = .) %>%
  tidy() 

mainEffect.acc.repnum <- mainEffect.acc %>% filter(term == 'repnum') 
 
boundary.acc <- relevantClicks %>% 
  unite("key", partnernum, repnum) %>%
  select(networkid, target, correct, key) %>%
  group_by(networkid, target, key) %>%
  mutate(gameid = row_number()) %>%
  spread(key, correct) %>% 
  select(networkid, target, gameid, `0_3`, `1_0`, `1_3`, `2_0`) %>%
  gather(jump, response, `0_3`:`2_0`) %>%
  mutate(jump = case_when(jump == '0_3' ~ 'first_end',
                          jump == '1_0'  ~ 'first_start',
                          jump == '1_3' ~ 'second_end',
                          jump == '2_0' ~ 'second_start')) %>%
  separate(jump, into = c('jumpID', 'timepoint')) %>%
  ungroup() %>%
  lme4::glmer(response ~ timepoint + jumpID + (1 | target) + (1 + timepoint | networkid) , 
        family = 'binomial',
        contrasts = list(jumpID = contr.sum),
        data = .) %>%
  tidy()

boundary.acc.timepoint <- boundary.acc %>% filter(term == "timepointstart")

increase.acc <- relevantClicks %>%  
  filter(repnum == 0) %>%
  lme4::glmer(correct ~ partnernum + (1  | participantid) + (1 |target),
              family = 'binomial',
              control = lme4::glmerControl("bobyqa"),
              data = .) %>%
  tidy()

increase.acc.partnernum <- increase.acc %>% filter(term == 'partnernum')
```

## Results

We evaluated participants' generalization behavior on the same three metrics we used in our simulations: accuracy, utterance length, and network convergence.

### Listener accuracy

We first examined changes in listener accuracy over different partners, where accuracy is defined as the proportion of trials where the target was correctly selected (see Fig. \ref{fig:results}A).
In particular, our partial pooling model predicts (1) gains in accuracy within each partner and (2) drops in accuracy at partner boundaries, but (3) overall improvement in initial interactions with successive partners.
To test the first prediction, we constructed a logistic mixed-effects regression predicting trial-level listener responses. 
We included a fixed effect of repetition block within partner (1, 2, 3, 4), along with random intercepts and slopes for each participant and each tangram. 
We found that accuracy improved over successive repetitions with every partner, $b =$ `r round(mainEffect.acc.repnum$estimate,2)`, $t =$ `r round(mainEffect.acc.repnum$statistic,2)`, $p<0.001$.

To test changes at partner boundaries, we constructed another regression model.
We coded the blocks before and after each partner swap and compared the final repetition block with one partner against the first repetition block with the next.
Because partner roles were randomized for each game, the same participant often did not serve as listener in both blocks, so in addition to tangram-level intercepts, we included random slopes and intercepts at the *network* level (instead of the participant level).
We found that across the two partner swaps, accuracy dropped significantly, $b=$ `r round(boundary.acc.timepoint$estimate,2)`, $t=$ `r round(boundary.acc.timepoint$statistic,2)`, $p=$ `r round(boundary.acc.timepoint$p.value, 3)`, reflecting the partner-specificity of meaning.
Finally, to test whether performance in the initial repetition block improves with subsequent partners, we examined the simple effect of partner number on first repetition block.
As predicted, we found a significant improvement in performance, $b=$ `r round(increase.acc.partnernum$estimate, 2)`, $t=$ `r round(increase.acc.partnernum$statistic, 2)`, $p=$ `r round(increase.acc.partnernum$p.value, 3)`, suggesting that listeners are bringing increasingly well-calibrated expectations into interactions with novel neighbors.

```{r}
# testing raw 
library(lmerTest)
mainEffect.reduction <- relevantMessages %>%  
  lmer(log(m) ~ repnum + (1 + repnum | participantid) + (1 + repnum |target),
              control = lme4::lmerControl("bobyqa"),
              data = .) %>%
  tidy() %>%
  filter(effect == 'fixed')
 
mainEffect.reduction.repnum <- mainEffect.reduction %>% filter(term == 'repnum') 
 
boundary.reduction <- relevantMessages %>% 
  unite("key", partnernum, repnum) %>%
  select(networkid, target, m, key) %>%
  group_by(networkid, target, key) %>%
  mutate(gameid = row_number()) %>%
  spread(key, m) %>% 
  select(networkid, target, gameid, `0_4`, `1_1`, `1_4`, `2_1`) %>%
  gather(jump, uttLength, `0_4`:`2_1`) %>%
  mutate(jump = case_when(jump == '0_4' ~ 'first_end',
                          jump == '1_1'  ~ 'first_start',
                          jump == '1_4' ~ 'second_end',
                          jump == '2_1' ~ 'second_start')) %>%
  separate(jump, into = c('jumpID', 'timepoint')) %>%
  ungroup() %>%
  lmer(log(uttLength) ~ timepoint + jumpID + (1 | target) + (1 + timepoint | networkid) , 
        contrasts = list(jumpID = contr.sum),
        data = .) %>%
  tidy()

boundary.reduction.timepoint <- boundary.reduction %>% filter(term == "timepointstart")

increase.reduction <- relevantMessages %>%  
  filter(repnum == 1) %>%
  lmer(log(m) ~ partnernum + (1  | participantid) + (1 |target),
    data = .) %>%
  tidy() 

increase.reduction.partnernum <- increase.reduction %>% filter(term == 'partnernum')
```

### Speaker utterance length

Next, as a measure of coding efficiency, we calculated the raw number of words produced by a speaker on each trial.
We then tested analogs of the same three predictions we tested in the previous section: speakers should *reduce* utterance length with each partner and revert to longer utterances at partner boundaries, but become gradually more willing to use shorter referring expressions with successive partners.
We tested these predictions using the same mixed-effects models, but using (log) utterance length as a continuous DV (see Fig. \ref{fig:results}B).
First, we found that speaker used fewer words over the course of interaction with every partner, $b =$ `r round(mainEffect.reduction.repnum$estimate,2)`, $t =$ `r round(mainEffect.reduction.repnum$statistic,2)`, $p<0.001$.
Second, we found that length increased across partner-boundaries, $b =$ `r round(boundary.reduction.timepoint$estimate,2)`, $t =$ `r round(boundary.reduction.timepoint$statistic,2)`, $p<0.001$, indicating speaker sensitivity to different partners.
Finally, we found an incremental decrease in the lengths of *initial descriptions* as speakers interacted with more partners on their network,$b =$ `r round(increase.reduction.partnernum$estimate,2)`, $t =$ `r round(increase.reduction.partnernum$statistic,2)`, $p<0.001$.

### Network convergence 

```{r}
convergence.lmer <- matches %>% 
  mutate(match = matchRate > 0) %>%
  glmer(match ~ partnernum * partnerType  + (1   | target) + (1 + partnernum * partnerType | networkid), 
        family = 'binomial',
        control = lme4::glmerControl("bobyqa"),
        data = .) %>%
  tidy() %>%
  filter(effect == 'fixed')

convergence.lmer.interaction <- convergence.lmer %>% filter(term == 'partnernum:partnerTypewithin')

# matches %>% 
#   mutate(match = matchRate > 0) %>%
#   glmer(match ~ repid + partnernum + partnerType + partnernum:partnerType + repid:partnerType + (1 + partnernum  | target) + (1 + partnernum * partnerType | networkid), 
#         family = 'binomial',
#         control = lme4::glmerControl("bobyqa"),
#         data = .) %>%
#   tidy()
# repid +
```

While coarse signatures of accuracy and utterance length are consistent with the predictions of our partial pooling model, it is possible that the network as a whole still fails to coordinate.
In this section, we examine the actual *content* of pacts and measure changes in alignment across the network. 
Specifically, we extend the 'exact matching' measure of alignment used in Simulation 3 to messier natural language production by examining the *intersection* of words produced by different speakers, excluding common stop words (e.g. 'the', 'both') to emphasize the core conceptual content of the pact.
We then constructed a mixed-effects logistic regression predicting whether this intersection was non-empty for each pair of utterances produced by speakers.

The main comparison of interest was between currently interacting participants (within dyad), and participants who are not interacting (across dyad).
Because the latter group is not aware of one another, if they nonetheless happen to tacitly be aligned, it provides evidence of network-wide coordination.
Our prediction thus concerns the interaction between pair type and partner number: within-pair alignment should stay consistently high while (tacit) alignment between non-interacting pairs will gradually increase as participants interact with more partners. 
We included these variables and their interaction as fixed effects, along with random intercepts for each tangram and maximal random effects for each network (i.e. intercept, both main effects, and the interaction).
We found a significant interaction ($b =$ `r round(convergence.lmer.interaction$estimate,2)`, $t =$ `r round(convergence.lmer.interaction$statistic,2)`, $p<0.001$; see Fig. \ref{fig:results}C), indicating that although different pairs in a network may initially use different labels, some of these labels begin to spread through the network over subsequent interactions. 

# Discussion

How do community-level conventions emerge from local interactions? 
In this paper, we suggested a partial pooling account of convention formation, formalized as a hierachical Bayesian model, where conventions represent the shared structure that agents can "abstract away" from partner-specific interactions.
Unlike complete pooling accounts, this model allows for partner-specific common ground to override existing community-wide expectations given sufficient experience with a partner, or in the absence of strong conventions.
Unlike no-pooling accounts, it allows networks to gradually converge on more efficient and accurate initial expectations about new partners.
We conducted a series of simulations demonstrating key model predictions about generalization behavior, and evaluated these predictions in a natural-language communication experiment where participants interacted with different neighbors on their network.

Hierarchical Bayesian models have several other properties of theoretical interest for convention formation that may be useful to evaluate in future work.
First, they offer a "blessing of abstraction" [@GoodmanUllmanTenenbaum11_TheoryOfCausality], where community-level conventions may be learned even with relatively sparse input from each partner, as long as there is not substantial variance in the population. 
Second, they are more robust to deviations than complete-pooling models relying on a fixed set of memory slots or a single mental lexicon. 
\aeg{dictionary's the wrong metaphor since lexical items aren't represented in a strictly ordered list, but in a network}
This robustness is due to their ability to 'explain away' outliers with partner-specific models without their community-level expectations being affected. 
They can therefore explain how speakers are able to emerge with conventional knowledge intact from an extended series of communicative interactions where they have adapted to children or non-native speakers. 
Finally, the deep connection between hierarchical Bayesian models and accounts of *meta-learning*, or learning to learn [e.g. @grant2018recasting], provides a useful set of tools to analyze conventions as the result of agents solving a meta-learning problem, adapting to each partner along the way.
\aeg{just for future thinking:  the same partial pooling could be very useful for capturing the conventions themselves across speakers: there are some language-wide conventions (e.g., SVO order, AN order, but they are generally overridden by some individual constructions: topicalization, cluster of "a-adjectives"}

The current work captures and quantifies incremental convergence within communities of four unfamiliar English speakers towards a set of shared language conventions.  
We recognize that real-world communities are more complex than this, however, as each speaker takes part in a number of subcommunities which vary in size and overlap. 
For example, we use partially distinct conventions depending on whether we are communicating with psychologists, friends from high school, bilinguals, or children, and we are able to comprehend certain conventions that we do not use ourselves. 
For future work using hierachical Bayesian models to address the full scale of an individual's  network of communities, additional social knowledge about these communities must be learned and represented in the generative model [e.g. @gershman_learning_2017]. 
\aeg{nice!}
Our results are a promising first step, providing evidence that hierarchical generalization may be a foundational cognitive building block for establishing conventionality at the group level.

<!-- # Acknowledgements -->

<!-- Place acknowledgments (including funding information) in a section at -->
<!-- the end of the paper. -->

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```
\normalsize
\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
