\documentclass[11pt, floatsintext]{apa6}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[outdir=./]{epstopdf}
%\DeclareGraphicsExtensions{.eps}
\usepackage{caption}
\captionsetup{font=footnotesize}

\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{textcomp}
\usepackage{lingmacros}
\usepackage[draft]{hyperref}
\usepackage{apacite}
\usepackage{listings}
\usepackage{multirow}
\usepackage{stfloats}
\usepackage{todonotes}
\usepackage{svg}
\usepackage{booktabs}
\usepackage{stmaryrd}

\synctex=1
\usepackage{soul}

\newcommand{\den}[1]{\ensuremath{\llbracket #1 \rrbracket}}

\newcommand{\KL}[2]{\ensuremath{D_{KL}({#1}\, \| \, {#2})}}
\newcommand{\E}[2]{\ensuremath{\mathbb{E}_{#1}\left [#2 \right]}}

\newenvironment{figurehere}
	{\def\@captype{figure}}
	{}

\usepackage{lipsum}
%\pagenumbering{gobble}
%\usepackage{apacite}

\linespread{1}


\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\graphicspath{{./figures/}}
 
 \definecolor{Green}{RGB}{10,200,100}
  \definecolor{Red}{RGB}{200,100,50}
\newcommand{\tlg}[1]{\textcolor{Green}{[tom: #1]}}  
\newcommand{\rdh}[1]{\textcolor{Red}{[rdh: #1]}}  


\makeatother

\title{From partners to populations: \\[.1em] A hierarchical Bayesian account of coordination and convention}
\shorttitle{Conventions}
\author{Various people}
\affiliation{Various Universities}

\abstract{Abstract TBD}

\keywords{Keywords TBD}

\authornote{This report is based in part on work presented at the 39th, 40th, and 42nd Conferences of the Cognitive Science Society \cite{hawkins_convention-formation_2017,hawkins_emerging_abstractions_2018,hawkins2020generalizing}. Correspondence should be addressed to Robert D. Hawkins, e-mail: rdhawkins@princeton.edu}

\begin{document}
\maketitle

\begin{quote}
%The speaker wants to be understood [...] %, so he intends to speak in such a way that he will be interpreted in a certain way. 
\emph{In order to judge how he will be interpreted, [the speaker] uses his starting theory of interpretation. % interpreter’s readiness to interpret along certain lines. %Central to this picture is what the speaker believes is 
%The speaker does not necessarily speak in such a way as to prompt the interpreter to apply this prior theory; he may deliberately dispose the interpreter to modify his prior theory. But the speaker’s view of the interpreter’s prior theory is not irrelevant to what he says, nor to what he means by his words; it is an important part of what he has to go on if he wants to be understood.
As speaker and interpreter talk, their ``prior'' theories become more alike; so do their ``passing'' theories. 
%The asymptote of agreement and understanding is when passing theories coincide. 
%But the passing theory cannot in general correspond to an interpreter’s linguistic competence. 
Not only does it have its changing list of proper names and gerrymandered vocabulary, but it includes every successful use of any other word or phrase, no matter how far out of the ordinary [...] 
%Every deviation from ordinary usage, as long as it is agreed on for the moment [...] is in the passing theory as a feature of what the words mean on that occasion. 
Such meanings, transient though they may be, are literal. \\-- \citeA{davidson_nice_1986}.}

\end{quote}

\input{Introduction}

\section{Convention formation as\\ Hierarchical Bayesian inference}

\input{ModelOverview}

\section{Phenomenon \#1: \\ Increasing efficiency in ad hoc convention formation}

\input{Reduction}

\section{Phenomenon \#2: \\Ad hoc conventions are shaped by context}

\input{ContextSensitivity}

\section{Phenomenon \#3: \\Generalization to new partners in a social network}

\input{Generalization}

\section{General Discussion}

\input{Conclusion}

\section{\bf Acknowledgments}
\small

\bibliography{ref}
\bibliographystyle{apacite}

\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0}
\setcounter{figure}{0}





\begin{table*}[th!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{}                                   & \textbf{Parameter}                                              & \multicolumn{1}{l}{\textbf{Example parameter settings}}                 \\ \midrule
\multirow{11}{*}{\textbf{Partner design}}    & \multirow{3}{*}{What feedback is provided?}                & - no feedback at all                                                  \\
                                            &                                                                 & - only correct/incorrect                                                   \\
                                            &                                                                 & - real-time responses from partner                                         \\ \cmidrule(l){2-3} 
                                            & \multirow{3}{*}{Are you playing with the same partner?}         & - same partner for whole game                                              \\
                                            &                                                                 & - swap out partners every round                                            \\
                                            &                                                                 & - swap after $k$ rounds                                                    \\ \cmidrule(l){2-3} 
                                            & \multirow{3}{*}{What do you know about your partner?}         & - anonymous stranger                                              \\
                                            &                                                                 & - stranger with perceptual information                                            \\
                                            &                                                                 & - close friend                                                    \\ \cmidrule(l){2-3}                                             
                                            & \multirow{2}{*}{How consistent are roles across repetitions?}   & - consistent director/matcher                                              \\
                                            &                                                                 & - alternate roles each round                                               \\ \midrule
\multirow{7}{*}{\textbf{Stimulus design}}   & \multirow{2}{*}{How familiar are targets?}                      & - very familiar: colors, household objects                                 \\
                                            &                                                                 & - not at all familiar: tangrams, novel line drawings                       \\ \cmidrule(l){2-3} 
                                            & \multirow{2}{*}{How complex are targets?}                       & - very complex: busy visual scenes, clips of music                         \\
                                            &                                                                 & - not at all complex: geometric drawings                                   \\ \cmidrule(l){2-3} 
                                            & \multirow{3}{*}{How consistent are targets across repetitions?} & - exact same image of object                                               \\
                                            &                                                                 & - different pose/view of same object                                       \\
                                            &                                                                 & - different objects from same neighborhood                                 \\ \midrule
\multirow{5}{*}{\textbf{Context design}}    & \multirow{2}{*}{How similar are distractors to the target?}     & - very similar: same basic-level category                                  \\
                                            &                                                                 & - not at all similar: other categories                                     \\ \cmidrule(l){2-3} 
                                            & What is the size of context?                                    & - between 2 and 21                                                         \\ \cmidrule(l){2-3} 
                                            & \multirow{2}{*}{How consistent is context across repetitions?}  & - exact same context each round                                            \\
                                            &                                                                 & - randomized context (sometimes far, sometimes close)                      \\ \midrule
\multirow{3}{*}{\textbf{Repetition design}} & How many repetitions per target?                                & - between 3 and 100                                                        \\ \cmidrule(l){2-3} 
                                            & \multirow{2}{*}{What is spacing between repetitions?}           & - block structure                                                          \\
                                            &                                                                 & - sequential structure with interspersed contexts                          \\ \midrule
\textbf{Modality design}                    & What medium is used for communication?                          & \begin{tabular}[c]{@{}l@{}}- text\\ - audio\\ - gesture\\ - drawing\end{tabular} \\ \bottomrule
\end{tabular}%
}
\caption{\normalfont{Proposed parameterization for repeated reference games, each of which theoretically impacts the formation of conventions.}}
\label{table:parameters}
\end{table*}

\normalsize

\section*{Appendix A: Details of RSA model}

Our setting poses several technical challenges for the Rational Speech Act (RSA) framework.
In this Appendix, we describe these challenges in more detail and justify our choices.
First, when we allow the full space of Boolean lexicons $\phi$, we must confront settings where an utterance $u$ does not apply to any of the objects under the given lexicon, i.e. where $\mathcal{L}_\phi(o, u) = 0$ for all $o\in \mathcal{C}$. 
In this case, the normalizing constant is zero, and the literal listener distribution is not well-defined.
A similar problem may arise when no utterance in the speaker's repertoire is true of the target, in which case the $S_1$ distribution is not well-defined.
Several solutions to this problem were outlined by \citeA{bergen_pragmatic_2016}.
One of these solutions is to use a `softer' semantics in the literal listener, where a Boolean value of false does not strictly rule out an object but instead assigns a low numerical score, e.g. 
$$\mathcal{L}_\phi(o,u) = \left\{ \begin{array} {rl} 1 & \textrm{if $o \in $\den{u}$_\phi$} \\ \epsilon & \textrm{o.w.} \end{array}\right.$$
Whenever there is at least one $o\in\mathcal{C}$ where $u$ is true, this formulation assigns negligible listener probability to objects where $u$ is false, but ensures that the normalization constant to is non-zero even when $u$ is false for all objects.

While this solution suffices for one-shot pragmatics under lexical uncertainty, where $\epsilon$ may be calibrated, it runs into several complications in an iterated setting.
At sufficiently high values of $w_L$ and $w_S$, numerical overflow at higher levels of recursion may lead to similarly problematic normalization constants as elements drop entirely out of the support.
We generalize the same solution by adding a noise model at every level of recursion.
That is, every agent chooses randomly with noise probability $\epsilon$, ensuring a non-zero floor on the likelihood of each element of the support.
Formally this corresponds to a mixture distribution, e.g. 
$$L_0^{\epsilon}(o|u,\phi) = \epsilon \cdot P_{unif}(o) + (1-\epsilon) \cdot L_0(o|u,\phi)$$

%At the same time, this `soft' semantics leads to unexpected and unintuitive consequences at the level of the pragmatic speaker. 
%After renormalization in $L_0$, an utterance $u$ that fails to refer to any object in context is also by definition equally \emph{successful} for all objects, leading to a uniform probability distribution.
%Consequently, when $S_1$ reasons about this listener, an utterance that is literally false in the lexicon may be preferred over some true utterances.

%We ensure that the normalization constant is well-defined  using another method.
%, we define (1) a `null' utterance, which is true of all objects, and (2) a `null' object, for which all utterances are true. 
%We add this fixed null utterance to every lexicon, and add this fixed null object to every context.
%Two consequences follow: (1) when a particular utterance does not apply to any object in context, it will still apply to the null object, assigning the true target a negligible probability of being chosen under the $L_0$ and (2) when no utterances apply to the target object, the speaker will fall back on the `null' utterance.
%Intuitively, the null utterance can be interpreted as choosing not to speak, and the null object can be interpreted as recognizing a failure to refer to anything.

Second, when the speaker and listener agents marginalize over their beliefs about $\phi_k$ in selecting actions (Eq. 5), a question arises over where exactly the expectation ought to be taken. 
In our formulation, the expectation is taken over the \emph{utility} each agent is using to act, i.e. if the speaker and listener utilities are defined to be 
$$
\begin{array}{rcl}
U_L(o;u, \phi_k) & = & w_L \cdot \log S_1(u|o, \phi_k)\\
U_S(u;o, \phi_k) & = & w_S \cdot \log L_1(o|u, \phi_k) - w_C \cdot c(u) \\
\end{array}
$$
then the expectation is taken as follow:
$$
\begin{array}{rcl}
L(o|u) & \propto & \exp\left\{\int P(\phi_k |D_k)\cdot U_L(u; o, \phi_k) \, d \phi_k\right\}\\
S(u|o) & \propto & \exp\left\{\int P(\phi_k | D_k) \cdot U_S(u; o, \phi_k) \, d \phi_k\right\} \\
\end{array}
$$
We may interpret this formulation as each agent choosing an action proportional to its expected utility across different possible values of $\phi_k$, weighted by the agent's current posterior beliefs about the value their partner is using.

One alternative, suggested by \citeA{bergen_pragmatic_2016}, is to assume the expectation take places at a single level of recursion, say the $L_1$, as above, and then derive the other agent's behavior by reasoning directly about this marginalized distribution, e.g.
$$
\begin{array}{rcl}
S_{alt1}(u|o) & \propto & \exp\left\{w_S \cdot \log L(o|u) - w_C \cdot c(u)\right\} \\
\end{array}
$$
This may be interpreted as an assumption on the part of the speaker that the listener is already accounting for their own uncertainty, and best responding to such a listener.
Isolating lexical uncertainty over $\phi$ to a single level of recursion is a natural formulation for one-shot pragmatic phenomena, where additional layers of recursion can build on top of this marginal distribution to derive implicatures.
However, the interpretation is messier for the multi-agent setting, since it (1) induces an asymmetry where one agent considers the other's uncertainty but not vice versa, and (2) requires the speaker to use their own current posterior beliefs to reason about the listener's marginalization.
A final variant is to place the expectation outside the listener distribution but inside the speaker's informativity term.
$$
\begin{array}{rcl}
L_{avg} & = & \int P(\phi_k | D_k) L(o|u, \phi_k) d\phi_k \\
S_{alt2}(u|o) & \propto & \exp\left\{w_S \cdot \log L_{avg}(o|u) - w_C \cdot c(u)\right\} \\
\end{array}
$$
The interpretation of this variant is that the speaker first derives a distribution representing how the listener would respond \emph{on expectation} and then computes their surprisal relative to this composite listener.
While this variant is able to derive the desired phenomena, it can be shown that it induces an unintuitive initial bias under a uniform lexical prior, since the logarithm cannot distribute over the integral in the normalization constant. 
This bias was most apparent in the case of context-sensitivity (Simulation 2.1), where utterances could apply at different levels of abstraction.
Mathematically, the difference between these possibilities is whether the speaker's uncertainty about $\phi_k$ goes inside the renormalization of $L(o|u)$ (as in $S_{alt1}$), outside the renormalization but inside the logarithm (as in $S_{alt2}$), or over the entire utility (as in our main formulation).
 

 \begin{figure*}
\centering
    \includegraphics[scale=.9]{arbitrariness_grid.pdf}
  \caption{Coordination success (simulation 1.1) across a range of parameter values. Columns represent memory discount factors $\beta$, rows represent optimality parameters $w_S = w_L$. Communicative success is achieved under a wide range of settings, but convergence is limited in some regimes. At high values of $\beta$, with no ability to discount prior evidence, accuracy asymptotes quickly below perfect coordination; at low $\alpha$, inferences are weaker and agent actions are noisier, limiting the ability to converge; finally, at low values of $\beta$, when prior evidence is forgotten too quickly, convergence interacts with $\alpha$: when $\alpha$ is too high, the latest evidence may overwhelm all prior evidence, preventing the accumulation of shared history. The agent noise model is set to $\epsilon = 0.01$ in all simulations.}
  \label{fig:arbitrariness_grid}
\end{figure*}

 \begin{figure*}
\centering
    \includegraphics[scale=.7]{conjunction_grid.pdf}
  \caption{Speaker efficiency (simulation 1.2) across a range of parameter values representing different weights on informativity and cost. Rows represent agent optimality $w_S = w_L$, columns represent costs $w_C$, and different memory discount factors $\beta$ shown in different colors. Agents converge on more efficient ad hoc conventions for a wide regime of parameters. Broadly, when utterance cost is more heavily weighted relative to informativity, the speaker will not produce longer utterances even at the beginning of the interaction; when informativity is more heavily weighted, the speaker continues to prefer longer utterances despite their cost. Reduction is found at the critical point between this tradeoff. The exception is at low values of $w_C$, where reduction is found even at higher optimality simply due to compression when re-normalizing the speaker distribution.}
  \label{fig:conjunction_grid}
\end{figure*}

 \begin{figure*}
\centering
    \includegraphics[scale=.9]{exp2-acc-grid_cleaned.pdf}
  \caption{Raw empirical accuracy distributions for context-sensitivity experiment. Each row represents how the accuracies of different games shift across the four quarters of the task for a given condition. Dotted vertical line represents chance accuracy (0.25), solid vertical line represents pre-registered convergence threshold (0.75).}
  \label{fig:full_accuracy_grid}
\end{figure*}

 \begin{figure*}
\centering
    \includegraphics[scale=.8]{reduction_grid_search.pdf}
  \caption{Speaker efficiency across partners (simulation 3.2) for a range of parameter values. Rows represent speaker optimality $w_S$, columns represent cost weight $w_c$.}
  \label{fig:partnerspecificity_grid}
\end{figure*}
\end{document}  

