%!TEX root = ../dissertation.tex
In this paper, we considered the computational challenge faced by agents trying to communicate in a variable and non-stationary landscape of meaning.
We advanced a Bayesian meta-learning approach  in which agents continually adapt their beliefs about the semantics used each partner, in turn.
We formalized this approach by integrating three core cognitive mechanisms in a probabilistic framework: (1) initial uncertainty about what a partner thinks words mean, (2) partner-specific adaptation based on observations of language use in context, and (3) hierarchical structure for graded generalization to new partners.
This unified model resolves several puzzles that have posed challenges from prior models, including (1) why referring expressions shorten over repeated interactions with the same partner, (2) why context shapes the level of abstraction of conventions with a partner, and (3) how partner-specific common ground may coexist with the emergence of conventions at the population level. 

We conclude by discussing several broader questions raised by the theoretical perspective we have advanced, and corresponding pathways for future work.

\paragraph{Implications for language acquisition}

Through one theoretical lens, our model provides an argument for the continuity of probabilistic models of language acquisition across development \cite<e.g.>{XuTenenbaum07_WordLearningBayesian,FrankGoodmanTenenbaum09_Wurwur}. 
In other words, we have aimed to generalize Bayesian models of word learning, traditionally used in developmental science, to explain the continual \emph{ad hoc} learning required by mature language users. 
Conversely, this suggests that the lexical learning mechanisms adults use to coordinate on conventions \emph{within} dyadic interactions may be the same as those supporting language acquisition more broadly.
Our hierarchical partial pooling model suggests a new emphasis on the role of partner-specificity and generalization in development.
Most laboratory tasks investigating cross-situational word learning in children use only use a single speaker, and even sophisticated models of cross-situational word learning \cite[e.g.]{FrankGoodmanTenenbaum09_Wurwur} collapse over \emph{who} is talking. 

Yet, as we have argued, there is substantial variability in systems of meaning across different speakers and contexts; calibrating one's lexical priors to reflect this variability is an important aspect of learning a language.
If the majority of child-directed speech only comes from a single primary caregiver, then the child may face a difficult generalization problem once they begin interacting with others. 
When first hearing a new word from a novel speaker, or a familiar word used in an unfamiliar way, children face the same inductive problem we have studied: it is unclear whether the new observation is a quirk of that particular speaker \emph{or} indicative of a globally shared convention. 
There may therefore be substantial path-dependence in acquisition, as children develop their lexical prior and become better attuned to the overall variability in the population \cite<see>[Chap. 6]{Clark09_FirstLanguageAcquisition}. 

Our work also suggests a new explanation for why young children struggle to coordinate on \emph{ad hoc} conventions with one another in repeated reference games \cite{GlucksbergKraussWeisberg66_DevoRefGames,KraussGlucksberg69_DevoReferenceGames,KraussGlucksberg77_SocialNonsocialSpeech}. 
When an experimenter feeds them messages produced by adult speakers in other games, they are able to maintain high accuracy even as the utterance reduce down to one- or two-word label. 
When Kindergarteners play with one another, however, they continue to make errors even after 15 repetitions.
Children as old as fifth grade only improved with assistance from the experimenter and never approached the perfect levels of adult performance. 
Instead of beginning with the long indefinite descriptions full of hedges and modifiers that adults provide, it was observed that children began with short, idiosyncratic descriptions like \emph{Mother's dress}. 

While these failures were widely attributed to limits on theory of mind use, this explanation has been complicated by findings that children cannot even interpret their own utterances after a delay \cite{asher1976children}, suggesting that the source of the problem is \emph{not} rigid adherence to one's own preferred label but instead a more broadly impoverished lexical prior for the novel object.
Recent work by \citeA{LeungEtAl20_Pacts} explored this hypothesis by observing parent-child interactions in a repeated reference task. 
The data suggested that, unlike pairs of children, parent-child pairs were able to successful coordinate on \emph{ad hoc} conventions.
Parents helped to interactively scaffold these conventions: younger children were more likely to adopt the labels used by their parents.
In other words, children may begin with more substantial lexical uncertainty over what labels may be appropriate than their parents, and consequently take their parents' usage as an authoritative cue to the underlying lexicon. 

While these preliminary data are intriguing, more work is needed to explore the role of inter-partner variability and partner-specificity in language learning. 
For example, is the child's lexical prior better viewed as a representation of others' beliefs about the lexicon, or as an (asocial) epistemic state? 
To what extent is the ability to retrieve or use this lexical prior constrained by theory of mind development? 
Are childrens' lexical priors impoverished simply because they have not obtained enough variability in their linguistic input, or is there a more fundamental representational constraint that prevents children from accounting for partner-specific differences? 
Even infants are sensitive to coarse social distinctions based on foreign vs. native language \cite{KinzlerDupouxSpelke07_LanguageGroups}, or accent \cite{KinzlerEtAl09_AccentRace}, but when do fully partner-specific representations develop?

\paragraph{Similarities and differences across communication modalities}

%\begin{quote}
%The oral modality is not well suited to conveying messages mimetically (i.e., iconically), even though that function is also important to human languages. This function is, however, very well served by the manual modality. \cite[p.155]{}
%\end{quote}

While we have focused primarily on verbal and textual communication channels, research on the dynamics of adaptation in other communication modalities, including graphical \cite{GarrodFayLeeOberlanderMacLeod07_GraphicalSymbolSystems,TheisenEtAl10_SystematicityArbitrariness,hawkins2019disentangling} and gestural \cite{FayListerEllisonGoldinMeadow13_GestureBeatsVocalization,bohn2019young} modalities, is important for our account in several ways.
First, it is a core claim of our hierarchical model that the basic cognitive mechanisms underlying adaptation and convention formation are domain-general.
In other words, there is nothing inherently special in our account about spoken or written language as far as our ability to coordinate . 
Any system that we use to communicate should display similar \emph{ad hoc} learning dynamics because in every case, agents are trying to infer the system of meaning being used by their partner.
Directly comparing behavior in repeated reference games across different modalities is therefore necessary to determine which adaptation effects, if any, are robust and attributable to modality-general mechanisms.

Second, at the same time, our hierarchical learning model claims a critical role for the priors we build up across interactions with many individuals.
We therefore predict that different communication modalities should display certain systematic differences due to the representational structure of the communication channel.
For example, in the verbal modality, the tangram shapes from \cite{ClarkWilkesGibbs86_ReferringCollaborative} are highly innominate -- most people do not have much experience naming or describing them with words, so the global prior is weak and local adaptation plays a greater role.
In the graphical modality, where communication takes place by drawing on a shared sketchpad, agents can be expected to have a stronger prior rooted in assumptions about shared perceptual systems and visual similarity \cite{fan2018common} -- drawing a quick sketch of the tangram's outline may suffice for understanding.
Other stimuli have precisely the opposite property: to distinguish between natural images of dogs, agents may have developed strong global conventions in the linguistic modality (e.g. `husky', `poodle', `pug', etc) but drawing the necessary fine distinctions in the graphical modality may be initially very costly for novices, requiring the formation of local conventions. 
Gesture also has its own distinctive prior, which also allows communicators to use time and the space around them to convey mimetic or depictive meanings that may be difficult to encode verbally or graphically \cite{goldin-meadow_role_1999,clark2016depicting,mcneill1992hand}. 

%If we adhered solely to the verbal modality, we would be limited to a fairly narrow range of stimuli (e.g. abstract shapes/tangrams) where behavior in the lab isn't totally dominated by strong prior conventions people bring into the interaction. 
%For example, similar phenomena Pictionary games where participants use a whiteboard to draw messages instead of an auditory or text-based channel \cite{GarrodFayLeeOberlanderMacLeod07_GraphicalSymbolSystems,TheisenEtAl10_SystematicityArbitrariness,hawkins2019disentangling}.

Another modality-based manipulation is to attempt to destroy or scramble any meaningful priors that people might carry into the social interaction.
For example, \citeA{Galantucci05_EmergenceOfCommunication} introduced a novel `seismograph' interface for communication -- a stylus that could be moved side-to-side or lifted up or down to make contact with the sketch pad while the vertical dimension drifted downward at a constant rate.
The resulting messages consequently look nothing like the usual kinds of symbols people create: the relationship between motor actions and perceptual output is broken such that executing a familiar movement for a symbol or numeral instead produces an odd, wavy scribble.
Despite the relative lack of priors on signal meanings in this medium, people were nevertheless able to converge on successful signaling systems in repeated reference games \cite{RobertsGalantucci12_DualityOfPatterning,RobertsEtAl15_IconocityOnCombinatoriality}.
Other novel modalities used in iterated reference games include a `whistle' language where movements along a vertical touch bar slider correspond to changes in pitch \cite{VerhoefRobertsDingemanse15_Iconicity} and a visual analog where movements along the slider were presented visually \cite{VerhoefEtAl16_TemporalLanguage}.
Our model ought to be able to account for production and comprehension across these modalities simply by exchanging its encoder and decoder components to use an appropriate prior.

\paragraph{The role of feedback and backchannels}

If adaptation is learning, then an important corollary of our model is that the extent to which partners adapt should depend critically on the quality of the data $D_i$ on which they are conditioning: $P(\mathcal{L}_i | \Theta, D_i)$.
Our simulations used the simplest source of feedback: the utterance and response in a reference game.
A key direction for future work is to account for richer forms of feedback.
For example, a key feature of dialogue is the capacity for a \emph{real-time back-channel}.
Either individual may say anything at any point in time, thus allowing for interjections (uh-huh, hmmm, huh?), clarification questions, and other listener-initiated forms of feedback. 
Without elaborating the generative model of the listener to include these verbal behaviors, we cannot explain the inferences a speaker will make upon hearing them.
Such an elaboration will thus be critical to explaining why listeners send fewer messages in response over time and what impact early listener responses on conventionalization. 

Additionally, this elaboration would allow our model capture early, important empirical results from \citeA{KraussWeinheimer64_ReferencePhrases}, which manipulated the feedback channel.
In one condition, participants were able to talk bidirectionally, and in another the channel was unidirectional: the speaker was unable to hear the listener's responses. 
This real-time feedback manipulation was crossed with a behavioral feedback manipulation where the experimenters intercepted the listener's responses: one group of speakers was told that their partner made the correct response 100\% of the trials (regardless of their real responses), while another was told on half of the trials that their partner made the incorrect response. 

Under our account, if the speaker is unsure how their longer descriptions are being interpreted -- unsure whether or not they can get away with shorter, more ambiguous expressions -- they may not have enough evidence about meanings to justify shorter utterances. 
Indeed, \citeA{KraussWeinheimer66_Tangrams} found that even when told that their partner was getting 100\% correct, entirely blocking the verbal feedback channel significantly limited the reduction effect. 
Speakers converged to utterances that were about twice as long -- twice as inefficient -- in the limit. 
Telling speakers that their partner was performing poorly also inhibited reduction as a main effect, though to a lesser extent. 
In the extreme case of trying to communicate to a listener who can't respond and appears to not understand, speaker utterance length actually increased with repetition after an early dip. 
\citeA{HupetChantraine92_CollaborationOrRepitition} found that in the \emph{complete} absence of feedback --- when the speaker is instructed to repeatedly refer to a set of objects for a listener who is not present and will do their half of the task offline --- there is also no reduction in message length. 

More graded disruptions of feedback seem to force the speaker to use more words overall but not to significantly change the rate of reduction. 
For example, \citeA{KraussBricker67_Delay} tested a transmission delay to temporally shift feedback and an access delay to block the onset of listener feedback until the speaker is finished. 
Later, \citeA{KraussEtAl77_AudioVisualBackChannel} replicated the adverse effect of delay but showed that undelayed visual access to one's partner cancelled out the effect and returned the number of words used to baseline. 
On the listener's part, too, the ability to actively \emph{give} feedback appears critical for coordination. 
\citeA{SchoberClark89_Overhearers} showed that even listeners who \emph{overheard} the entire game were significantly less accurate than listeners who could directly interact with the speaker, even though they heard the exact same utterances.
Our model provides an initial framework to begin understanding how these subtle manipulations of feedback channels license differing inferences about a partner's underlying system of meaning.

\paragraph{What is being adapted?}

While our model has been formulated in terms of coordination on \emph{lexical} meaning, this is only one of many levels at which conventions may form. 
In more complex circumstances, there is often initial uncertainty not just about which of a small set of targets a particular message refers to, but how to represent the relevant targets of reference in the first place. 
Learning to communicate effectively may require discovering a lower-dimensional representation in which the targets of reference vary.
For instance, when using sketches to communicate about the identity of complex pieces of music \cite{HealeySwobodaUmataKing07_GraphicalLanguageGames}, a particular set of strokes could correspond to any number of properties (pitch, tempo, melody, rhythm, intensity) at any temporal granularity. 
This is made particularly clear in a classic maze game \cite{GarrodAnderson87_SayingWhatYouMean}: in order to give effective spatial directions, speakers apparently had well-tuned lexical priors but had to coordinate on what space of \emph{referents} to use (e.g. paths, coordinates, lines, landmarks). 

By appealing to classic spreading-activation connectionist models \cite<e.g.>{roelofs1992spreading}, interactive alignment accounts \cite{pickering2004toward} have argued that activating phonetic or syntactic features that are associated with specific lemmas in the lexicon can percolate to strengthen higher semantic levels of representation \cite{pickering1998representation}.
Thus, unconsciously coupling word choices \cite{louwerse2012behavior}, syntax \cite{gruberg2019syntactic,levelt1982surface}, body postures \cite{lakin2003using}, speech rate \cite{giles1991contexts}, or even informational complexity \cite{abney2014complexity} in dialogue could potentially contribute to the coordination of higher-level semantic representations.

Other theories have assumed representations of lexical meanings are relatively fixed and the only learning taking place is how one's partner construes a multi-stable percept. 
For examine, this seems to be what \citeA{BrennanClark96_ConceptualPactsConversation} had in mind when they coined the term \emph{conceptual pact}, and \cite{stolk2016conceptual} have influentially argued that partners in communication construct shared conceptual spaces. 
Given present data it is not clear how these two sources of uncertainty could be teased apart, though certain kinds of conventions (e.g. proper names or acronyms) seem to rely more on binding new linguistic tokens to meanings than on constructing new conceptualizations.
Thus, we expect both levels of coordination are likely to play an important role. 
Our probabilistic model could be extended to handle additional levels of coordination by placing uncertainty over a hyper-parameter corresponding to the intended feature dimension that must be jointly inferred with the correspondence along that dimension. 

%\rdh{TODO: paragraph citing Hawkins, Kwon, Sadigh, Goodman to argue that gradient-based meta-learning with neural network representation is an alternative architecture that is more computationally tractable at scale than a fully probabilistic model?}

\paragraph{Additional layers of social structure}

Real-world communities are much more complex than the simple networks we considered: each speaker takes part in a number of overlapping subcommunities. 
For example, we use partially distinct conventions depending on whether we are communicating with psychologists, friends from high school, bilinguals, or children \cite{auer_code-switching_2013}.
For instance, when a scientist is talking to other scientists about their work, they know they can use efficient technical shorthand that they would avoid when talking to their non-expert friends and family. 
Previous work has probed representations of community membership by manipulating the extent to which cultural background is shared between speaker and listener.
For example, \citeA{IsaacsClark87_ReferencesExpertsNovices} paired participants who had either lived in NYC or had never been there for a task referring to landmarks in the city (e.g. ``Rockefeller Center''). 
Within just a few utterances from a novel partner, people could infer whether they were playing with an expert or novice and immediately adjust their language use to be appropriate for this inferred identity. 
Social information about a partner’s group can be so important that even players in artificial-language games react to the restrictions of social anonymity by learning to identify members of their community using distinctive signals \cite{roberts_experimental_2010}.

For future work using hierarchical Bayesian models to address the full scale of an individual's  network of communities, additional social knowledge about these communities must be learned and represented in the generative model.
Larger-scale networked experiments can be used to evaluate the hypothesis that a hierarchical representation of conventions includes not just a partner-specific level and population-wide level but also intermediate community levels. 
This hypothesis can be formalized by including additional latent representations of community membership into our hierarchical model.
That is, in addition to updating our model of a particular \emph{partner} based on immediate feedback, even sparse observations of a partner's language use may license much broader inferences about their lexicon via diagnostic information about their social group or background. 
If someone's favorite song is an obscure B-side from an 80s hardcore band, you can make fairly strong inferences about what else they like to listen to and how similar they might be to you \cite{VelezEtAl16_Overlaps, GershmanEtAl17_StructureSocialInfluence}. 
Similarly, if someone casually refers to an obscure New York landmark you also recognize, you can safely update your beliefs about their lexicon to include a number of other conventions shared among New Yorkers. 
Lexica cluster within social groups, so inverting this relationship can yield rapid lexical learning from inferences about social group membership.

This explanation is also consistent with broader linguistic phenomena outside the realm of repeated reference games. 
For example, \citeA{PottsLevy15_Or} showed that lexical uncertainty is critical for capturing constructions like \emph{oenophile or wine lover}, where a disjunction of synonymous terms is taken to convey a definition -- information about the speaker's lexicon -- rather than a disjoint set. 
While the reasons that speakers produce such constructions are complex, we would expect that speakers will be more likely to produce the definitional \emph{or} when the component word is expected to be rarer or more obscure for a particular partner: when there is additional uncertainty over its likely meaning in the listener's lexicon.

\paragraph{Conclusion}

Language is not a rigid body of knowledge that we acquire at an early age and deploy mechanically for the rest of our lives. 
Nor is its evolution a slow, inter-generational drift. 
It is a means for communication -- a shared interface between minds -- and must therefore adapt over the rapid timescales required by communication. 
In other words, we are constantly learning language. 
Not just one language, but a family of related languages, across every repeated interaction with every partner. 

\begin{quote}
Let us conclude not that ‘there is no such thing as a language’ that we bring to interaction with others. Say rather that there is no such thing as the one total language that we bring. We bring numerous only loosely connected languages from the loosely connected communities that we inhabit. \citeA{hacking1986nice}
\end{quote}
