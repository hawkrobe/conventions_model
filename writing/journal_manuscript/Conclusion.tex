%!TEX root = ../dissertation.tex
Communication in a variable and non-stationary landscape of meaning creates unique computational challenges.
To address these challenges, we advanced a hierarchical Bayesian approach  in which agents continually adapt their beliefs about the form-meaning mapping used by each partner, in turn.
We formalized this approach by integrating three core cognitive capacities in a probabilistic framework: representing initial uncertainty about what a partner thinks words mean \textbf{(C1)}, partner-specific adaptation based on observations of language use in context \textbf{(C2)}, and hierarchical structure for graded generalization to new partners \textbf{(C3)}.
This unified model resolves several puzzles that have posed challenges from prior models of coordination and convention formation: why referring expressions shorten over repeated interactions with the same partner \textbf{(P1)}, how partner-specific common ground may coexist with the emergence of conventions at the population level \textbf{(P2)}, and how context shapes which conventions emerge \textbf{(P3)}.

We conclude by raising five broader questions that follow from the theoretical perspective we have advanced, each suggesting pathways for future work: (1) are the mechanisms underlying \emph{ad hoc} convention formation in adults the same as those underlying language acquisition in children? (2) to what extent do these mechanisms depend on the communication modality? (3) what kinds of feedback are used to coordinate on conventions? (4) what kinds of social structure are reflected in the hierarchical prior, and (5) which representations are involved in adaptation at a process-level?

\subsection{Continuity of language learning across development}

There is a close mathematical relationship between our model of convention formation and recent probabilistic models of word learning in developmental science \cite<e.g.>{XuTenenbaum07_WordLearningBayesian,FrankGoodmanTenenbaum09_Wurwur}. 
This similarity suggests an intriguing conjecture that the continual-learning mechanisms adults use to rapidly coordinate on partner-specific conventions may be the same as those supporting lexical acquisition in children.
In other words, people may never stop learning language; they may simply develop stronger and better-calibrated beliefs about their community's conventions, which apply to a broader swath of communicative scenarios.
In this section, we discuss three possible implications of viewing language development in terms of social coordination and convention.

First, common paradigms for cross-situational learning typically assume a fixed speaker and focus on variability across referential contexts \cite{siskind1996computational,regier2005emergence,smith2014unrealized,yurovsky2015integrative}. 
Yet, as we have argued, variability in how words are used by different \emph{partners} may pose an equally challenging problem.
While there has been limited work on the social axis of generalization, it is increasingly apparent that children are able to track \emph{who} produced the words they are learning and use this information to determine whether word meanings should generalize not just to other contexts but also to other speakers.
For example, young children may limit the generalizability of observations from speakers who use language in idiosyncratic ways, such as a speaker who call a ball a ``dog'' \cite{koenig2010sensitivity,luchkina2018eighteen}, and even retrospectively update beliefs about earlier words after observing such idiosyncracies \cite{dautriche2021}.

This discounting of idiosyncratic speakers may be understood as an instance of the same inductive problem that convention formation poses for adults in \textbf{P2}.
Unlike complete-pooling models, which predict that all observations should be taken as equally informative about the community's conventions, our hierarchical model predicts that children should be able to explain away ``outliers'' without their community-level expectations being disrupted.
Indeed, a novel prediction generated by our account is that children should be able to accommodate idiosyncratic language \emph{within} extended interaction with the same speaker (e.g. continue to pretend the ball is called ``dog,'' given partner-specific common ground) while also limiting generalization of that convention \emph{across} other speakers.

Second, we have emphasized the importance of representing lexical \emph{uncertainty}, capturing expected variability in the population beyond the point estimates assumed by traditional lexical representations. 
But how do children calibrate their lexical uncertainty?
One possibility assigns a key role to the number of distinct speakers in a child's environment, by analogy to the literature on talker variability \cite{creel2011talker,clopper2004effects}.
Exposure to fewer partners may result in weaker \cite<e.g.>{lev2017talking} or mis-calibrated priors for meanings.
If an idiosyncratic construction is over-represented in the child's environment, they may later be surprised to find that it was specific to their household's lexicon and not shared by the broader community  \cite<see>[Chap. 6]{Clark09_FirstLanguageAcquisition}. 
Conversely, however, hierarchical inference predicts a blessing of abstraction \cite{GoodmanUllmanTenenbaum11_TheoryOfCausality}: under certain conditions, community-level conventions may be inferred even with relatively short sparse observations from each partner.
To resolve these questions, future work will need to develop new methods for eliciting children's expectations about partner-specificity and variability of meanings.

Third, our work suggests a new explanation for why young children may struggle to coordinate on \emph{ad hoc} conventions with one another in repeated reference games \cite{GlucksbergKraussWeisberg66_DevoRefGames,KraussGlucksberg69_DevoReferenceGames,KraussGlucksberg77_SocialNonsocialSpeech}. 
%When an experimenter feeds them messages produced by adult speakers in other games, they are able to maintain high accuracy even as the utterance reduce down to one- or two-word label. 
%When Kindergarteners play with one another, however, they continue to make errors even after 15 repetitions.
Children as old as fifth grade only improve with assistance from the experimenter \cite{matthews2007toddlers}: instead of beginning with the long indefinite descriptions that adults provide, it was observed that children began with shorter descriptions like \emph{Mother's dress} \cite<see also>{kempe2019adults}.
While these failures were initially attributed to limits on theory of mind use, this explanation has been complicated by findings that children cannot even interpret their own utterances after a delay \cite{asher1976children}. 
In other words, errors are not well-explained by egocentric adherence to a strongly preferred label, or downstream failures to acknowledge that this preferred label may not be understood by their partner; there appears to be no such preferred label.

Our model raises the possibility that the problem may instead stem from production with an impoverished lexical prior: there are no existing conventions for such a novel object in their vocabulary \cite{goldberg2019explain}, leaving their preferences dispersed widely over ``good-enough'' constructions.
Indeed, when children are paired with their caregivers rather than peers, they are able to successfully form conventions \cite{LeungEtAl20_Pacts}.
Parents helped to interactively scaffold these conventions, both by proactively seeking clarification in the listener role \cite<e.g.>{anderson1994interactive} and by providing more descriptive labels in the speaker role, which children adopted themselves on later trials.
These results highlight one way in which our model of convention formation may coincide with models of language acquisition in development: in both cases, listeners are trying to infer the meanings of words in the speaker's lexicon \cite{bohn2019pervasive}. 

While we have highlighted three particular developmental phenomena where our approach may generate novel predictions, there are many finer-grained questions raised by our computational approach.
For example, is the child's lexical expectations in communication best explained as a (resource-limited) representation of \emph{others'} lexicons, or as an egocentric (asocial) epistemic state? 
To what extent is the ability to retrieve or use this lexical prior constrained by theory of mind development? 
Even infants are sensitive to coarse social distinctions based on foreign vs. native language \cite{KinzlerDupouxSpelke07_LanguageGroups}, or accent \cite{KinzlerEtAl09_AccentRace}, but when do fully partner-specific representations develop?

\subsection{The role of communication modality}

%\begin{quote}
%The oral modality is not well suited to conveying messages mimetically (i.e., iconically), even though that function is also important to human languages. This function is, however, very well served by the manual modality. \cite[p.155]{}
%\end{quote}

While we have focused primarily on verbal and textual communication channels, there has beens significant progress understanding the dynamics of adaptation in other communication modalities, including graphical \cite{GarrodFayLeeOberlanderMacLeod07_GraphicalSymbolSystems,TheisenEtAl10_SystematicityArbitrariness,hawkins2019disentangling} gestural \cite{FayListerEllisonGoldinMeadow13_GestureBeatsVocalization,motamedi2019evolving,bohn2019young} and other \emph{de novo} modalities \cite{Galantucci05_EmergenceOfCommunication,RobertsGalantucci12_DualityOfPatterning,RobertsEtAl15_IconocityOnCombinatoriality,VerhoefRobertsDingemanse15_Iconicity,VerhoefEtAl16_TemporalLanguage,kempe2019adults}.
These modalities are important for our account in several ways.

Most importantly, it is a core claim of our hierarchical account that the basic learning mechanisms underlying adaptation and convention formation are domain-general.
In other words, we predict that there is nothing inherently special about spoken or written language: any system that humans use to communicate should display similar \emph{ad hoc} learning and convention formation dynamics because in every case people are simply trying to infer the system of meaning being used by their partner.
Directly comparing behavior in repeated reference games across different modalities is therefore necessary to determine which adaptation effects, if any, are robust and attributable to modality-general mechanisms.

At the same time, our hierarchical learning model predicts a critical role for the \emph{priors} we build up across interactions with many individuals.
We therefore predict that different communication modalities should display certain systematic differences due to the representational structure of the communication channel.
For example, in the verbal modality, the tangram shapes from \citeA{ClarkWilkesGibbs86_ReferringCollaborative} are highly ``innominate'' \cite{HupetEtAl91_CodabilityReference} -- most people do not have much experience naming or describing them with words, so their priors are weak and local adaptation plays a greater role.
In the graphical modality, where communication takes place by drawing on a shared sketchpad, people can be expected to have a stronger prior rooted in assumptions about shared perceptual systems and visual similarity \cite{fan2018common} -- drawing a quick sketch of the tangram's outline may suffice for understanding.

Other referents have precisely the opposite property: to distinguish between natural images of dogs, people may have strong existing conventions in the linguistic modality (e.g. `husky', `poodle', `pug') but making the necessarily fine-grained visual distinctions in the graphical modality may be initially very costly for novices \cite{fan2020pragmatic}, requiring the formation of local conventions to achieve understanding \cite{hawkins2019disentangling}. 
The gestural modality also has its own distinctive prior, which also allows communicators to use time and the space around them to convey mimetic or depictive meanings that may be difficult to encode verbally or graphically \cite{goldin-meadow_role_1999,clark2016depicting,mcneill1992hand}. 
We suggest that differences in production and comprehension across modalities may therefore be understood by coupling modality-specific priors with modality-generic learning mechanisms.

%If we adhered solely to the verbal modality, we would be limited to a fairly narrow range of stimuli (e.g. abstract shapes/tangrams) where behavior in the lab isn't totally dominated by strong prior conventions people bring into the interaction. 
%For example, similar phenomena Pictionary games where participants use a whiteboard to draw messages instead of an auditory or text-based channel \cite{GarrodFayLeeOberlanderMacLeod07_GraphicalSymbolSystems,TheisenEtAl10_SystematicityArbitrariness,hawkins2019disentangling}.

%Another modality-based manipulation is to attempt to destroy or scramble any meaningful priors that people might carry into the social interaction.
%For example, \citeA{Galantucci05_EmergenceOfCommunication} introduced a novel `seismograph' interface for communication -- a stylus that could be moved side-to-side or lifted up or down to make contact with the sketch pad while the vertical dimension drifted downward at a constant rate.
%The resulting messages consequently look nothing like the usual kinds of symbols people create: the relationship between motor actions and perceptual output is broken such that executing a familiar movement for a symbol or numeral instead produces an odd, wavy scribble.
%Despite the relative lack of priors on signal meanings in this medium, people were nevertheless able to converge on successful signaling systems in repeated reference games \cite<see also>{RobertsGalantucci12_DualityOfPatterning,RobertsEtAl15_IconocityOnCombinatoriality}.
%Other novel modalities used in iterated reference games include a `whistle' language where movements along a vertical touch bar slider correspond to changes in pitch \cite{VerhoefRobertsDingemanse15_Iconicity} and a visual analog where movements along the slider were presented visually \cite{VerhoefEtAl16_TemporalLanguage}.

\subsection{The role of feedback and backchannels}

If convention formation is grounded in inference, then an important corollary of our model is that the extent to which partners are able to coordinate should depend critically on the observations $D_i$ they condition on in Eq.~\ref{eq:joint_inference}.
In the \emph{complete} absence of feedback --- when the speaker is instructed to repeatedly refer to a set of objects for a listener who is not present and will do their half of the task offline --- there is no reduction in message length \citeA{HupetChantraine92_CollaborationOrRepitition}. 
Our simulations examined the most minimal sources of feedback: the speaker's utterance and the listener's response in a reference game.
A key direction for future work is to account for richer forms of feedback that arise in natural communication.
For example, a key feature of dialogue is the capacity for a \emph{real-time back-channel}.
The listener may say anything at any point in time, thus allowing for interjections (uh-huh, hmmm, huh?), clarification questions, and other listener-initiated forms of feedback. 
Elaborating the generative model of the listener to include these verbal behaviors will be critical to explaining other key results from \citeA{ClarkWilkesGibbs86_ReferringCollaborative}, including changes in the frequency of listener-initiated feedback over the course of interaction.

Additionally, such an elaboration would allow our model to address early empirical results from \citeA{KraussWeinheimer66_Tangrams} manipulating the feedback channel: participants were able to talk bidirectionally in one condition but in another condition, the channel was unidirectional. 
The speaker was prevented from hearing the listener's responses. 
This feedback manipulation was crossed with a behavioral feedback manipulation where the experimenters intercepted the listener's responses: one group of speakers was told that their partner made the correct response 100\% of the trials (regardless of their real responses), while another was told on half of the trials that their partner made the incorrect response. 
Our account of increasing efficiency (\textbf{P1}) predicts that speaker may not have sufficient evidence to justify shorter utterances in the absence of evidence about how their longer descriptions are being interpreted.
Indeed, \citeA{KraussWeinheimer66_Tangrams} found that both channel contribute independently to gains in efficiency.
Speakers kept using longer utterances when they observed that their partner was making errors. 
But blocking the real-time verbal back-channel significantly limited reduction, even when told that their partner was achieving perfect accuracy.
%Speakers converged to utterances that were about twice as long as those found when verbal feedback was allowed.
In the extreme case of trying to communicate to a listener who can't respond and also appears to not understand, speaker utterance length actually \emph{increased} with repetition after an early dip. 

More graded disruptions of feedback seem to force the speaker to use more words overall but not to significantly change the rate of reduction. 
For example, \citeA{KraussBricker67_Delay} tested a transmission delay to temporally shift feedback and an access delay to block the onset of listener feedback until the speaker is finished. 
Later, \citeA{KraussEtAl77_AudioVisualBackChannel} replicated the adverse effect of delay but showed that undelayed visual access to one's partner cancelled out the effect and returned the number of words used to baseline. 
On the listener's part, too, the ability to actively \emph{give} feedback appears critical for coordination. 
\citeA{SchoberClark89_Overhearers} showed that even listeners who \emph{overheard} the entire game were significantly less accurate than listeners who could directly interact with the speaker, even though they heard the exact same utterances, presumably because the speaker was not able to take their particular sources of confusion into account.
Our model provide a formal framework to begin probing how these different forms of communicative feedback may license different inferences about one's partner in interaction.

\subsection{The role of social knowledge}

Real-world communities are much more complex than the simple networks we considered: each speaker takes part in a number of overlapping subcommunities. 
For example, we use partially distinct conventions depending on whether we are communicating with psychologists, friends from high school, bilinguals, or children \cite{auer_code-switching_2013}.
When a scientist is talking to other scientists about their work, they know they can use efficient technical shorthand that they would avoid when talking to their non-expert friends and family. 
Previous work has probed representations of community membership by manipulating the extent to which cultural background is shared between speaker and listener.
For example, \citeA{IsaacsClark87_ReferencesExpertsNovices} paired participants who had either lived in NYC or had never been there for a task referring to landmarks in the city (e.g. ``Rockefeller Center''). 
Within just a few utterances from a novel partner, people could infer whether they were playing with an expert or novice and immediately adjust their language use to be appropriate for this inferred identity. 
Social information about a partner’s group can be so important that even players in artificial-language games react to the restrictions of social anonymity by learning to identify members of their community using distinctive signals \cite{roberts_experimental_2010}.

For future work using hierarchical Bayesian models to address the full scale of an individual's  network of communities, additional social knowledge about these communities must be learned and represented in the generative model.
Larger-scale networked experiments can be used to evaluate the hypothesis that a hierarchical representation of conventions includes not just a partner-specific level and population-wide level but also intermediate community levels. 
This hypothesis can be formalized by including additional latent representations of community membership into our hierarchical model.
That is, in addition to updating our model of a particular \emph{partner} based on immediate feedback, even sparse observations of a partner's language use may license much broader inferences about their lexicon via diagnostic information about their social group or background. 
If someone's favorite song is an obscure B-side from an 80s hardcore band, you can make fairly strong inferences about what else they like to listen to and how similar they might be to you \cite{VelezEtAl16_Overlaps, GershmanEtAl17_StructureSocialInfluence}. 
Similarly, if someone casually refers to an obscure New York landmark, you may be able to update your beliefs not just about that lexical item but about a number of other lexical conventions shared among New Yorkers. 
Lexica cluster within social groups, so inverting this relationship can yield rapid lexical learning from inferences about social group membership.

This explanation is also consistent with broader linguistic phenomena outside the realm of repeated reference games. 
For example, \citeA{PottsLevy15_Or} showed that lexical uncertainty is critical for capturing constructions like \emph{oenophile or wine lover}, where a disjunction of synonymous terms is taken to convey a definition -- information about the speaker's lexicon -- rather than a disjoint set. 
While the reasons that speakers produce such constructions are complex, we would expect that speakers will be more likely to produce the definitional \emph{or} when the component word is expected to be rarer or more obscure for a particular partner: when there is additional uncertainty over its likely meaning in the listener's lexicon.


\subsection{Process-level mechanisms for adaptation}

Finally, while we have provided a computational-level account of coordination and convention formation in terms of hierarchical inference, there remain many possible process-level mechanisms that may perform this computation.
In this section, we discuss two interlocking process-level questions: (1) exactly which representations are being adapted? (2) how does our model scale to larger spaces of utterances and referents? %and (3) what memory systems are required for tracking and generalizing partner-specific meanings?

\subsubsection{Which representations are adapted?}
While our model formulation focused on adaptation at the level of the lexicon (i.e. inferences about $\phi$, representing different possible lexical meanings), this is only one of many internal representations that may need to be adapted to achieve successful coordination. 
Three other possible representational bases have been explored in the literature. 

First, it is possible that adaptation takes place upstream of the lexicon, directly implicating perceptual or \emph{conceptual} representations \cite{GarrodAnderson87_SayingWhatYouMean,HealeySwobodaUmataKing07_GraphicalLanguageGames}
That is, there may also be uncertainty about how a particular partner construes the referent itself, and communication may require constructing a shared, low-dimensional conceptual space where the relevant referents can be embedded \cite{stolk2016conceptual}.
This is particularly clear in the classic maze task \cite{GarrodAnderson87_SayingWhatYouMean} where giving effective spatial directions requires speakers to coordinate on what spatial representations to use (e.g. paths, coordinates, lines, or landmarks). 

Second, it is possible that adaptation takes place even further upstream, at the level of \emph{social} representations \cite{jaech2018low}.
Rather than directly updating beliefs about lexical or conceptual representations, we may update a holistic representation of the partner themselves (e.g. as a ``partner embedding'' in a low-dimensional vector space) that is used to retrieve downstream conceptual and lexical representations. 
Under this representational scheme, the mapping from the social representation to particular conventions is static, and \emph{ad hoc} adaptation is limited to learning where a particular partner belongs in the overall social space.

Third, it is possible that expectations about other lower-level features of a partner are also adapted through interaction.
For example, interactive alignment accounts \cite{pickering2004toward} have argued that activating phonetic or syntactic features that are associated with specific lemmas in the lexicon can percolate up to strengthen higher levels of representation \cite{roelofs1992spreading,pickering1998representation}.
Thus, learning about a partner's word frequency \cite{louwerse2012behavior}, syntax \cite{gruberg2019syntactic,levelt1982surface}, body postures \cite{lakin2003using}, speech rate \cite{giles1991contexts}, or even informational complexity \cite{abney2014complexity} could be functionally useful for communication if they covary with, or are informative about, higher-level representations. 

\subsubsection{Scalability to larger utterance and referent spaces}

While a fully Bayesian formulation elegantly captures the inference problem at the core of our theory, the posterior update step in Eq.~\ref{eq:joint_inference} grows increasingly intractable as the space of utterances and referents grows. 
This computational limitation is especially evident when considering how to quantitatively fit models to the natural-language data produced in unconstrained reference games, or applications in modern machine learning where we would like to build artificial agents that can adapt to human partners.
Generalizing our framework to the arbitrary natural language (e.g. referring expressions using the full vocabulary of an adult language user) and arbitrary visual input (e.g. sensory impressions of novel objects such as tangrams) that are the basis for natural communication not only requires a different representational space for language and visual input, it may require different inference algorithms. 

The first computational obstacle is the lexical representation.
A discrete $m\times n$ matrix containing entries for each utterance-referent pair, as typically used in convention formation simulations, has two primary limitations as a proxy for human representations: (1) it  grows quadratically as additional utterances and referents are added while becoming increasingly sparse (i.e. most words only apply to a relatively small set of referents) and (2) it does not straightforwardly represent similarity relationships between utterances and referents (i.e. a new referent must be added as a distinct new column, even if it is visually similar to a known referent).
This limitation is especially clear when referents are presented as raw images. 

The second computational obstacle is the inference algorithm.
Even if a more scalable family of lexical representations $\mathcal{L}_\phi$ is chosen, any parameterization $\phi$ of this representation will be much higher-dimensional than we have considered.
For example, if $(\Theta, \phi)$ are defined to be the weights of a neural network, then maintaining uncertainty $P(\Theta, \phi)$ would require placing a prior over the weights and the posterior update $P(\Theta, \phi | D)$ would require a difficult integral over a very high-dimensional support. 
This is called a (hierarchical) Bayesian neural network \cite{mackay1992practical,neal2012bayesian}.
While recent algorithmic breakthroughs have made (approximate) inference for such networks increasingly tractable \cite<e.g.>{hernandez2015probabilistic,lacoste2018uncertainty,dusenberry2020efficient,izmailov2020subspace}, they remain untested as cognitive models.

While these obstacles are significant, we argue that our hierarchical Bayesian framework is nonetheless well-poised to explain coordination and convention-formation at scale. 
In particular, recent formal connections between hierarchical Bayes and gradient-based meta-learning approaches in machine learning \cite{grant_recasting_2018} suggest an alternative algorithm that relaxes the full prior over $\Theta$ to a point estimate and replaces the difficult integral in the posterior update with a handful of (regularized) gradient steps. 
This more tractable algorithm approximates the same computational-level problem we have formulated but provides a different perspective: conventions are learned \emph{initializations} and coordination is partner-specific \emph{fine-tuning} or \emph{domain adaptation} of vector representations. 
The fine-tuning approach, initializing with a state-of-the-art neural language model, has recently accounted for psycholinguistic data on reading times \cite{van2018neural} as well as \emph{ad hoc} convention formation in reference games using (unseen) natural images as referents \cite{hawkins2019continual}.

%\subsection{Memory systems supporting generalization}
%
% \cite{horton_revisiting_2016}.


\paragraph{Conclusion}

We have argued that successful communication depends on continual learning across multiple timescales. 
We must coordinate on \emph{ad hoc} meaning through common ground with individual partners but also abstract these experiences away to represent stable, generalizable \emph{conventions} and norms.
Like other socially-grounded knowledge, language is not a rigid dictionary that we acquire at an early age and deploy mechanically for the rest of our lives. 
Nor does language only change over the slow time-scales inter-generational drift.
Language is a means for communication -- a shared interface between minds -- and must therefore adapt over the rapid contextual timescales required by communication.
As new \emph{ad hoc} concepts arise, new \emph{ad hoc} conventions must be formed to solve the new coordination problems they pose.
In other words, we are constantly learning language. 
Not just one language, but a family of related languages, across interactions with each partner. 

\begin{quote}
\emph{Let us conclude not that ‘there is no such thing as a language’ that we bring to interaction with others. Say rather that there is no such thing as the one total language that we bring. We bring numerous only loosely connected languages from the loosely connected communities that we inhabit.} \cite{hacking1986nice}
\end{quote}
