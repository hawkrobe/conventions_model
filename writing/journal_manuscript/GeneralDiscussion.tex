\subsection{Communities}
\todo[inline]{from 2018}
How and why do abstractions emerge in local interactions? We hypothesized that although communicative contexts requiring fine distinctions would favor one-to-one object-word mappings, pressures for efficiency would allow abstractions to emerge in coarser contexts. By manipulating context statistics in a real-time experiment, we found evidence for these pragmatic influences on interactive convention formation.

Our results may help to illuminate the relationship between our concepts and words, which are often treated interchangeably. While our mental taxonomies are adaptive to the natural perceptual structure of the world \cite{MervisRosch81_CategorizationReview} %(Rosch et al, 1976; Mervis \& Rosch ,1981; Murphy \& Smith, 1982), 
it is far from inevitable that all levels of these conceptual hierarchies become conventionalized as lexical items. There are many perfectly natural concepts that are not represented by distinct words in the English language: for instance, we do not have words for each tree in our yards, or for ad-hoc concepts %like \emph{things to sell at a garage sale} 
\cite{Barsalou83_AdHocCategories}. Indeed, English speakers are often fascinated by foreign words like the Danish ``hygge'' (a specific notion of coziness) or Scottish ``tartle'' (hesitating when introducing someone because you've forgotten their name) that are difficult to express in English.
Our results highlight communicative needs to distinguish, in context, as a force behind the choice to lexicalize some fine-grained concepts. 
A related direction for future work is to explore the relationship between communicative need and \emph{basic-level} structure.

While we showed how abstract words emerge from efficiency even in a task requiring only reference to individual objects, there are other clear functional advantages to having abstract terms in the lexicon. For one, they allow speakers to efficiently refer to large, potentially infinite, sets of things, and make generalizations about categories, e.g.\ ``Dogs bark'' \cite{TesslerGoodman16_Generics}. Future work should explore this as an additional pressure toward abstract, nested nouns.
Similarly, the option to refer to more specific concepts with compound terms (e.g.~``spotted dog''), which was not available in our experiment, may impact final conventions.
We expect that labels will become lexicalized when the cost incurred by frequently using a compositional construction exceeds the cost of adding an additional word to the lexicon. 
Future work should also explore these hypotheses about how lexicalization of nominal terms trades off with compositionality. 


%\todo[inline]{question for NDG: Should we comment on some methodlogical things people put in the `strategy' box? e.g.\ some people seem to be taking notes or using sound-symbolic associations like "nogo" to mean "red" (not a problem; should just help memory or help make initial meanings not entirely arbitrary but shouldn't change contextual demands\dots)}

%\todo[inline]{question for MF: put some broader comment connecting to/contrasting with other cognitive theories of what's going on here? e.g.\ (1) not a simple associative learning account, where people just store memory traces of word-object co-occurances (can't account for context effects of generation), although note that probably a simpler model just counting co-occurences could account for the post-test responses just as well (it just wouldn't have as flexible an underlying representation). (2) elaborates on Xu \& Tenenbaum (2007) word learning story by introducing contextual pressures?}
%\ndg{could do if there is a short clear point to make...}
%\mf{phew, not sure there is enough space and hesitant that this might wake sleeping dogs, no matter how careful we are}


Finally, although we implemented a purely statistical Bayesian data analysis model to infer lexica, it is also possible to consider a cognitive model of participants' own lexical inferences. Indeed, our findings are consistent with a recent cognitive model of convention formation which explained the rapid coordination on efficient but informative lexical terms as a process of mutual lexical learning \cite{HawkinsFrankGoodman17_ConventionFormation}. %Instead of simply using RSA as a linking function to analyze the snapshot meanings without assuming anything about how \emph{agents} might be learning or developing their lexica in response their partner -- . 
In this model, each agent assumes their partner is rationally producing cooperative utterances under some latent lexicon; given initial uncertainty over the contents of that lexicon, agents can invert their model of their partner to infer their lexicon from observable behavior. 
The different dynamics we observed across conditions, then, may be the consequence of different lexical inferences in different local contexts. Further, while we used RSA as a linking function in our statistical model, a cognitive model would allow us to test to what extent pragmatic reasoning is necessary to explain behavior.% may result from the different inferences made by pragmatic agents about the lexicon.  % \cite{BergenLevyGoodman16_LexicalUncertainty}. %Each individual is trying to infer the lexicon being used by their partner by observing their behavior in context. 

Our shared lexical conventions are richly structured systems with meanings at multiple levels of abstraction. There is now abundant evidence that languages adapt to the needs of their users, and the context-sensitive emergence of abstractions demonstrated in this paper suggests that the driver of this adaptation may lie in the remarkably rapid adaptability of agents themselves. We are constantly supplementing our existing language with local conventions as we need them. Our separate minds may organize the world into meaningful conceptual hierarchies but our shared language only evolves to reflect this structure when it is communicatively relevant. 

\todo[inline]{from 2020}
How do community-level conventions emerge from local interactions? 
In this paper, we proposed a partial-pooling account, formalized as a hierarchical Bayesian model, where conventions represent the shared structure that agents "abstract away" from partner-specific interactions.
Unlike complete-pooling accounts, this model allows for partner-specific common ground to override community-wide expectations given sufficient experience with a partner, or in the absence of strong conventions.
Unlike no-pooling accounts, it allows networks to converge on more efficient and accurate expectations about novel partners.
We conducted a series of simulations demonstrating the model's generalization behavior, and evaluated these predictions with a natural-language communication experiment on a small network.

Hierarchical Bayesian models have several other properties of theoretical interest for convention formation.
First, they offer a "blessing of abstraction" \cite{GoodmanUllmanTenenbaum11_TheoryOfCausality}, where community-level conventions may be learned even with relatively sparse input from each partner, as long as there is not substantial variance in the population. 
Second, they are more robust to partner-specific deviations from conventions (e.g. interactions with children or non-native speakers) than complete-pooling models relying on a fixed set of memory slots or a single mental 'inventory.' 
This robustness is due to their ability to 'explain away' outliers without community-level expectations being affected. 
Finally, the deep connection between hierarchical Bayesian models and accounts of *meta-learning*, or learning to learn \cite{grant_recasting_2018}, provides a useful set of tools to analyze conventions as the result of agents solving a meta-learning problem, adapting to each partner along the way.

Real-world communities are much more complex than the simple networks we considered: each speaker takes part in a number of overlapping subcommunities. 
For example, we use partially distinct conventions depending on whether we are communicating with psychologists, friends from high school, bilinguals, or children.
For future work using hierarchical Bayesian models to address the full scale of an individual's  network of communities, additional social knowledge about these communities must be learned and represented in the generative model \cite[e.g.]{gershman_learning_2017} 
Our results are a promising first step, providing evidence that hierarchical generalization may be a foundational cognitive building block for establishing conventionality at the group level.
